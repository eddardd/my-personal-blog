{
  
    
        "post0": {
            "title": "Optimal Transport for Domain Adaptation",
            "content": "The idea of this blog post is to give a deeper idea on how to perform Optimal Transport for Domain Adaptation. Throughout this tutorial, we will use the works of Courty et al. [1], who were the first authors to propose using Optimal Transport for adapting models in a unsupervised way. . We do not give much details on how to perform Optimal Transport. We simply rely on the awesome library Python Optimal Transport. For more information you can take a look on the textbook by Peyré and Cuturi [2]. In this tutorial, we explore how to adapt a Convolutional Neural Network for classifying slightly different digits. . !pip install pot !pip install torchinfo . Collecting pot Downloading POT-0.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (664 kB) |████████████████████████████████| 664 kB 7.7 MB/s Requirement already satisfied: scipy&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from pot) (1.4.1) Requirement already satisfied: numpy&gt;=1.16 in /usr/local/lib/python3.7/dist-packages (from pot) (1.21.6) Installing collected packages: pot Successfully installed pot-0.8.2 Collecting torchinfo Downloading torchinfo-1.6.5-py3-none-any.whl (21 kB) Installing collected packages: torchinfo Successfully installed torchinfo-1.6.5 . . import ot import numpy as np import matplotlib.pyplot as plt from tqdm import tqdm import torch from torchinfo import summary from torchvision import datasets from torchvision import transforms from torchvision.utils import make_grid from sklearn.metrics import accuracy_score from sklearn.neighbors import KNeighborsClassifier . device = &#39;cpu&#39; . Loading the datasets . For the transfer learning task, we will use MNIST [3] and USPS [4] datasets. These are very simillar datasets which consists on handwritten digits (in white) on a solid background (in black). Our goal is to adapt a model learned on MNIST to classify digits in USPS correctly. . This adaptation problem has been extensively studied by the community (e.g. [5, 6]). Here we provide a show case that does not focuses on performance. We perform the following preprocessing steps to the digits: . We convert each pixel from uint8 [0, 255] to float32 in [0, 1]. This is done by the transforms.ToTensor() call, as you can see below. | We resize each image to $32 times 32$. Note that this introduces artifacts, especially in USPS digits which were $16 times 16$ originally. | We replicate the image accross the 3 RGB channels | This is the step-by-step of the transform composition shown below. . T = transforms.Compose([ transforms.ToTensor(), transforms.Resize((32, 32)) ]) . Once the preprocessing pipeline is constructed, we can create source and target dataloaders . src_dataset = datasets.MNIST(root=&#39;./.tmp&#39;, train=True, transform=T, download=True) src_loader = torch.utils.data.DataLoader(dataset=src_dataset, batch_size=256, shuffle=True) tgt_dataset = datasets.USPS(root=&#39;./.tmp&#39;, train=True, transform=T, download=True) tgt_loader = torch.utils.data.DataLoader(dataset=tgt_dataset, batch_size=256, shuffle=True) . Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./.tmp/MNIST/raw/train-images-idx3-ubyte.gz Extracting ./.tmp/MNIST/raw/train-images-idx3-ubyte.gz to ./.tmp/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./.tmp/MNIST/raw/train-labels-idx1-ubyte.gz Extracting ./.tmp/MNIST/raw/train-labels-idx1-ubyte.gz to ./.tmp/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./.tmp/MNIST/raw/t10k-images-idx3-ubyte.gz Extracting ./.tmp/MNIST/raw/t10k-images-idx3-ubyte.gz to ./.tmp/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./.tmp/MNIST/raw/t10k-labels-idx1-ubyte.gz Extracting ./.tmp/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./.tmp/MNIST/raw Downloading https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2 to ./.tmp/usps.bz2 . . We can see a few samples from both datasets by iterating through the dataloaders . for xs, ys in src_loader: break for xt, yt in tgt_loader: break . fig, axes = plt.subplots(1, 2, figsize=(10, 5)) ind = np.random.choice(np.arange(len(xs)), size=64) samples = xs[ind] grid = make_grid(samples).numpy().transpose([1, 2, 0]) print(&#39;Source Labels: {}&#39;.format(ys[ind].reshape(8, 8))) axes[0].set_title(&#39;Source Domain Samples&#39;) axes[0].imshow(grid) axes[0].axis(&#39;off&#39;) ind = np.random.choice(np.arange(len(xs)), size=64) samples = xt[ind] grid = make_grid(samples).numpy().transpose([1, 2, 0]) print(&#39;Target Labels: {}&#39;.format(yt[ind].reshape(8, 8))) axes[1].set_title(&#39;Target Domain Samples&#39;) axes[1].imshow(grid) axes[1].axis(&#39;off&#39;) . Source Labels: tensor([[8, 6, 6, 2, 5, 7, 9, 2], [3, 1, 2, 8, 6, 3, 9, 1], [6, 9, 8, 1, 7, 7, 2, 4], [8, 9, 4, 5, 3, 6, 6, 1], [1, 9, 1, 9, 0, 0, 8, 9], [7, 2, 2, 9, 4, 2, 3, 6], [3, 0, 4, 6, 4, 4, 2, 4], [4, 7, 0, 4, 9, 4, 0, 6]]) Target Labels: tensor([[9, 0, 0, 0, 1, 1, 1, 9], [5, 7, 5, 7, 2, 4, 1, 9], [0, 3, 2, 9, 3, 0, 4, 8], [2, 0, 2, 7, 0, 1, 6, 3], [1, 2, 2, 5, 9, 3, 3, 0], [9, 4, 4, 8, 1, 2, 3, 0], [0, 0, 1, 7, 6, 3, 2, 0], [6, 7, 6, 9, 8, 1, 4, 8]]) . (-0.5, 273.5, 273.5, -0.5) . Note that even though much simillar, the source and target datasets are quite different. Especially, MNIST digits are centered on the grid $32 times 32$, while USPS digits tend to occupy the whole grid. As it turns out this is quite important for the CNN, despite not interfering with human recognition. . This is an example of the known covariate shift phenomenon, which corresponds to when $P_{S}(X) neq P_{T}(X)$. In other words, the marginal feature distribution changes accross domains. This happens when the statistical properties of the data change. . In what follows, we will, . Load a CNN feature extractor and train it on MNIST | Measure its performance on USPS | Use Optimal Transport to enhance the performance on USPS. | . Loading a pretrained a Feature Extractor . Here we use the classical architecture LeNet5 [6], especifically designed for classifying MNIST digits. Here you can see its overall description. . . We code this network architecture using Pytorch&#39;s Module API. . class LeNet5(torch.nn.Module): def __init__(self, n_channels=3): self.n_channels = n_channels super(LeNet5, self).__init__() self.feature_extractor = torch.nn.Sequential( torch.nn.Conv2d(in_channels=n_channels, out_channels=6, kernel_size=5), torch.nn.ReLU(), torch.nn.MaxPool2d(stride=2, kernel_size=2), torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5), torch.nn.ReLU(), torch.nn.MaxPool2d(stride=2, kernel_size=2) ) self.class_discriminator = torch.nn.Sequential( torch.nn.Linear(in_features=16 * 5 * 5, out_features=120), torch.nn.ReLU(), torch.nn.Linear(in_features=120, out_features=84), torch.nn.ReLU(), torch.nn.Linear(in_features=84, out_features=10), torch.nn.Softmax(dim=1) ) def forward(self, x): y = self.feature_extractor(x) h = y.view(-1, 16 * 5 * 5) features = self.class_discriminator[:-2](h) predicted_labels = self.class_discriminator(h) return features, predicted_labels . note that our neural network architecture is now a class. To start using the model, we need to instantiate it into an object, . model = LeNet5(n_channels=1) . To further understand the architecture of LeNet5, we use torchinfo, which further shows the number of parameters in the network. . summary(model, input_size=(100, 1, 32, 32)) . ========================================================================================== Layer (type:depth-idx) Output Shape Param # ========================================================================================== LeNet5 -- -- ├─Sequential: 1-1 [100, 16, 5, 5] -- │ └─Conv2d: 2-1 [100, 6, 28, 28] 156 │ └─ReLU: 2-2 [100, 6, 28, 28] -- │ └─MaxPool2d: 2-3 [100, 6, 14, 14] -- │ └─Conv2d: 2-4 [100, 16, 10, 10] 2,416 │ └─ReLU: 2-5 [100, 16, 10, 10] -- │ └─MaxPool2d: 2-6 [100, 16, 5, 5] -- ├─Sequential: 1 -- -- │ └─Linear: 2-7 [100, 120] 48,120 │ └─ReLU: 2-8 [100, 120] -- │ └─Linear: 2-9 [100, 84] 10,164 │ └─ReLU: 2-10 [100, 84] -- ├─Sequential: 1-2 [100, 10] -- │ └─Linear: 2-11 [100, 120] (recursive) │ └─ReLU: 2-12 [100, 120] -- │ └─Linear: 2-13 [100, 84] (recursive) │ └─ReLU: 2-14 [100, 84] -- │ └─Linear: 2-15 [100, 10] 850 │ └─Softmax: 2-16 [100, 10] -- ========================================================================================== Total params: 61,706 Trainable params: 61,706 Non-trainable params: 0 Total mult-adds (M): 48.13 ========================================================================================== Input size (MB): 0.41 Forward/backward pass size (MB): 5.21 Params size (MB): 0.25 Estimated Total Size (MB): 5.87 ========================================================================================== . Training the model consists on minimizing the Cross Entropy between the ground truth class vector (i.e. to which digit an image corresponds) and the predictions. This loss function is given by, . $$ mathcal{L}(y, hat{y}) = - dfrac{1}{n} sum_{i=1}^{n} sum_{j=1}^{K}y_{ij} log hat{y}_{ij} $$where $n=256$ is the number of samples in the minibatch and $K$ is the number of classes in our problem. An important remark must be done here. During training we assume that labels come in an one hot encoding format. This means that each categorical value (i.e. 1, ..., 10) is transformed in a 10-dimensional vector, with 1 on the respective position of its class. . Therefore, the ground-truth $y$ is now a matrix $y in mathbb{R}^{n times K}$, as well as the predictions $ hat{y}$. Each vector $ hat{y}_{i}$ contains the network&#39;s predictions for the $i$-th sample, which is a softmax/probability distribution over classes. . criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) . last_loss = 0 for it in range(10): accs, losses = [], [] pbar = tqdm(src_loader) for x, y in pbar: optimizer.zero_grad() # Get data and pushes into device x, y = x.to(device), y.to(device) y = torch.nn.functional.one_hot(y, num_classes=10).float() # Forward pass _, yhat = model(x) # Evaluate loss loss = criterion(yhat, y) # Backward pass loss.backward() # Optimization step optimizer.step() # Append losses to list it_loss = loss.cpu().item() losses.append(it_loss) # Append accs to list it_acc = accuracy_score(y.detach().argmax(dim=1).cpu().numpy(), yhat.detach().argmax(dim=1).cpu().numpy()) accs.append(it_acc) pbar.set_description(&#39;Loss: {}, Acc: {}&#39;.format(it_loss, it_acc)) loss = np.mean(losses) acc = np.mean(accs) δloss = abs(loss - last_loss) print(&#39;It {}/{}, loss: {}, acc: {}, δloss: {}&#39;.format(it + 1, 10, loss, acc, δloss)) last_loss = loss . Loss: 1.524747371673584, Acc: 0.9479166666666666: 100%|██████████| 235/235 [00:28&lt;00:00, 8.21it/s] . It 1/10, loss: 1.7222665279469591, acc: 0.75348515070922, δloss: 1.7222665279469591 . Loss: 1.56523859500885, Acc: 0.875: 100%|██████████| 235/235 [00:24&lt;00:00, 9.76it/s] . It 2/10, loss: 1.533622589517147, acc: 0.9313829787234043, δloss: 0.1886439384298122 . Loss: 1.5207856893539429, Acc: 0.9375: 100%|██████████| 235/235 [00:23&lt;00:00, 9.81it/s] . It 3/10, loss: 1.5086321825676776, acc: 0.9556515957446808, δloss: 0.0249904069494693 . Loss: 1.4832005500793457, Acc: 0.9791666666666666: 100%|██████████| 235/235 [00:24&lt;00:00, 9.65it/s] . It 4/10, loss: 1.4972100871674558, acc: 0.9657358156028369, δloss: 0.011422095400221766 . Loss: 1.4630451202392578, Acc: 1.0: 100%|██████████| 235/235 [00:24&lt;00:00, 9.44it/s] . It 5/10, loss: 1.4906726801649053, acc: 0.9717918882978723, δloss: 0.006537407002550566 . Loss: 1.4850155115127563, Acc: 0.9791666666666666: 100%|██████████| 235/235 [00:24&lt;00:00, 9.73it/s] . It 6/10, loss: 1.487435332257697, acc: 0.9749445921985816, δloss: 0.00323734790720831 . Loss: 1.4866124391555786, Acc: 0.96875: 100%|██████████| 235/235 [00:24&lt;00:00, 9.74it/s] . It 7/10, loss: 1.4847782921283803, acc: 0.9772606382978724, δloss: 0.002657040129316668 . Loss: 1.4855841398239136, Acc: 0.9791666666666666: 100%|██████████| 235/235 [00:24&lt;00:00, 9.71it/s] . It 8/10, loss: 1.482060980796814, acc: 0.980114140070922, δloss: 0.0027173113315663855 . Loss: 1.4614702463150024, Acc: 1.0: 100%|██████████| 235/235 [00:24&lt;00:00, 9.44it/s] . It 9/10, loss: 1.4801266025989614, acc: 0.9816489361702128, δloss: 0.001934378197852471 . Loss: 1.4897629022598267, Acc: 0.9791666666666666: 100%|██████████| 235/235 [00:24&lt;00:00, 9.69it/s] . It 10/10, loss: 1.4783393408389802, acc: 0.9836546985815604, δloss: 0.001787261759981229 . . Measuring the Model Performance . A standard case in domain adaptation is the naive approach: we do not try to adapt the model. This corresponds to applying the classifier to a dataset that follows a different probability distribution -- and hence may not work as well as the source dataset on which the classifier was trained. This is known in the literature as the baseline. . Here we proceed as in [1], and evaluate the baseline with a 1-NN classifier on the features extracted by the CNN. Our first step is to construct a matrix $H_{S} in mathbb{R}^{n_{S} times 84}$, which serves as the training data for the 1-NN. We do so by defining . $$h_{S}^{i} = phi(x_{S}^{i})$$ . where $ phi$ is the convolutional feature extractor. . Hs, Ys = [], [] for xs, ys in tqdm(src_loader): with torch.no_grad(): hs, _ = model(xs) Hs.append(hs) Ys.append(ys) Hs = torch.cat(Hs, dim=0) Ys = torch.cat(Ys) . 100%|██████████| 235/235 [00:15&lt;00:00, 15.10it/s] . We perform a similar operation on the target domain . Ht, Yt = [], [] for xt, yt in tqdm(tgt_loader): with torch.no_grad(): ht, _ = model(xt) Ht.append(ht) Yt.append(yt) Ht = torch.cat(Ht, dim=0) Yt = torch.cat(Yt) . 100%|██████████| 29/29 [00:01&lt;00:00, 15.71it/s] . Finally, the 1-NN procedure consists on calculating the pairwise distances between source and target domain data. This is done through the function cdist of Pytorch. . C = torch.cdist(Hs, Ht, 2) ** 2 ind_opt = C.argmin(dim=0) del C . Yp = Ys[ind_opt] . print(accuracy_score(Yt, Yp)) . 0.7151282402962557 . This will serve as a baseline for the performance of domain adaptation. In this case, a succesfull adaptation implies on an accuracy higher than 71.51% . Optimal Transport . Background . Optimal Transport (OT) [7] is a mathematical theory concerned with mass displacement at least effort. The field was founded by the French mathematician Gaspard Monge, and recast in the 20-th century in terms of linear programming, by nobel prize Leonid Kantorovich. . But, what makes OT interesting for ML, and more specifically Transfer Learning? Well, one can understand probability distributions as distributions of mass. This way, OT serve as a framework for manipulating probability distributions. This is of key importance in many fields of ML (e.g. Generative Modeling), especially transfer learning. . To set up the OT problem, let us consider hypothetical, unknown probability distributions $P_{S}(X)$ and $P_{T}(X)$ from which source and target features are drawn from. These can be approximated empirically from samples, $$ hat{P}_{S}(x) = dfrac{1}{n_{S}} sum_{i=1}^{n_{S}} delta( mathbf{x}- mathbf{x}_{S}^{i}),$$ where $ delta$ is the Dirac delta function. For data matrices $ mathbf{X}_{S} in mathbb{R}^{n_{S} times d}$ and $ mathbf{X}_{T} in mathbb{R}^{n_{T} times d}$, the OT problem revolves around findng a transportation plan $ pi in mathbb{R}^{n_{S} times n_{T}}$, that specieis how much mass is transported between samples $ mathbf{x}_{S}^{i}$ and $ mathbf{x}_{T}^{j}$. As such, $ pi$ should conserve mass, that is, it should not create nor distruct mass through transportation. This amounts to imposing, $$ sum_{i=1}^{n_{S}} pi_{ij} = dfrac{1}{n_{T}} text{ and } sum_{j=1}^{n_{T}} pi_{ij} = dfrac{1}{n_{S}}. $$ . These are linear constraints imposed on $ pi_{ij}$. If we further define a cost of transportation $c( cdot, cdot)$ between $ mathbf{x}_{S}^{i}$ and $ mathbf{x}_{T}^{j}$, we can define an objective function that corresponds to the total effort $E( pi)$, . $$E( pi) = sum_{i=1}^{n_{S}} sum_{j=1}^{n_{T}} pi_{ij}c( mathbf{x}_{S}^{i}, mathbf{x}_{T}^{j})$$ . The OT problem is thus a linear program, which means that its cost and constraints are linear with respect to the variables $ pi_{ij}$. This is, nonetheless, a large-scale problem, as the number of variables scales with the number of samples. In the era of Deep Learning, this is a huge constraint as the number of samples in datasets is remarkably huge. We will get back to this matter in a bit. . Entropic Regularization . Instead of using linear programming for solving OT, we follow [8] and calculate an efficient approximation of $ pi$ by introducing an entropic regularization term to $E$. This consists on changing the objective function to, . $$E( pi) = sum_{i=1}^{n_{S}} sum_{j=1}^{n_{T}} pi_{ij}c( mathbf{x}_{S}^{i}, mathbf{x}_{T}^{j}) + epsilon sum_{i=1}^{n_{S}} sum_{j=1}^{n_{T}} pi_{ij} log pi_{ij}$$ . In practice, this has two effects: (i) it smoothes the linear programming problem, leading to a smooth $ pi$. (ii) it allows for a faster calculation based on matrix-scaling algorithms. These 2 factors often lead to better adaptation results. . Optimal Transport for Domain Adaptation . Even though $ pi$ is informative about source and target distributions, it does not tells us how to map different samples. This is solved by introducing the concept of barycentric mapping, which is defined as, . $$T_{ pi}( mathbf{x}_{S}^{i}) = underset{ mathbf{x} in mathbb{R}^{d}}{ text{argmin}} sum_{j=1}^{n_{T}} pi_{ij}c( mathbf{x}, mathbf{x}_{T}^{j}).$$ . When $c( mathbf{x}_{S}^{i}, mathbf{x}_{T}^{j}) = lVert mathbf{x}_{S}^{i} - mathbf{x}_{T}^{j} rVert_{2}^{2}$, this problem has closed form solution, . $$T_{ pi}( mathbf{X}_{S}) = n_{S} pi mathbf{X}_{T}$$ . however, note that $T_{ pi}$ is only defined for the source domain samples it has been defined on. For our particular dataset, this would amount to calculating an OT plan of size $60000 times 7291$, which can be done, but would take an enormous amount of time and storage. . A workaround was introduced by [9], which consists on downsampling the datasets (to a sub-set of representative points), fitting the map $T_{ pi}$ on this sub-sample, then interpolating $T_{ pi}$ on novel samples, . $$T_{ pi}( mathbf{x}) = T_{ pi}( mathbf{x}_{S}^{i_{ star}}) + mathbf{x} - mathbf{x}_{S}^{i_{ star}}$$ . where $1 leq i_{ star} leq n_{S}$ is the index of the nearest neighbor $ mathbf{x}$ in the data matrix $ mathbf{X}_{S}$. . Extracting data . As mentioned previously, we cannot fit OT to the entire dataset. We thus extract a sub-sample from the original datasets in order to have a feasible $T_{ pi}$. As follows, we extract 10 batches from the source and target datasets. . _Hs, _Ys = [], [] k = 0 for xs, ys in src_loader: with torch.no_grad(): hs, _ = model(xs) _Hs.append(hs) _Ys.append(ys) k += 1 if k == 10: break _Hs = torch.cat(_Hs, dim=0).numpy() _Ys = torch.cat(_Ys, dim=0).numpy() . _Ht, _Yt = [], [] k = 0 for xt, yt in tgt_loader: with torch.no_grad(): ht, _ = model(xt) _Ht.append(ht) _Yt.append(yt) k += 1 if k == 10: break _Ht = torch.cat(_Ht, dim=0).numpy() _Yt = torch.cat(_Yt, dim=0).numpy() . _Hs.shape, _Ht.shape, _Ys.shape, _Yt.shape . ((2560, 84), (2560, 84), (2560,), (2560,)) . Fitting the Barycentric Mapping . Here we use Python Optimal Transport for fitting $T_{ pi}$ to our sample. . otda = ot.da.SinkhornTransport(reg_e=1e-2, norm=&#39;max&#39;) otda.fit(Xs=_Hs, ys=None, Xt=_Ht, yt=None) . /usr/local/lib/python3.7/dist-packages/ot/bregman.py:517: UserWarning: Sinkhorn did not converge. You might want to increase the number of iterations `numItermax` or the regularization parameter `reg`. warnings.warn(&#34;Sinkhorn did not converge. You might want to &#34; . &lt;ot.da.SinkhornTransport at 0x7f282dcdf0d0&gt; . . next we visualize the optimal transport plan, . plt.imshow(np.log(otda.coupling_ + 1e-12), cmap=&#39;Reds&#39;) . &lt;matplotlib.image.AxesImage at 0x7f28286a5b50&gt; . Note that we cannot see much structure in $ pi$. This is because source and target samples are not ordered with respect to their classes. Intuitively, samples within the same class are close to each other (e.g. ones in MNIST are closer to ones in USPS), thus we expect $ pi$ to be class sparse. This notion was introduced in [1], and corresponds to the following: . $$ pi_{ij} neq 0 iff y_{S}^{i} neq y_{T}^{j}$$ . Of course, this property is not properly enforced as we did not use the label information when fitting $ pi$. If we sort the rows and columns of $ pi$ by the label vectors, we can see if this property is respected, . plt.imshow(np.log(otda.coupling_[_Ys.argsort(), :][:, _Yt.argsort()] + 1e-12), cmap=&#39;Reds&#39;) . &lt;matplotlib.image.AxesImage at 0x7f28298ddc50&gt; . note that $ pi$ is approximately class-sparse, reflecting the fact that features are somewhat enough to induce this desirable property. . Why Class Sparsity? . Right now, you may be wondering why class sparsity is important. Well, let us take a look on where samples $ mathbf{x}_{S}^{i}$ are transported to. For a given $i$, it is mapped to, . $$ hat{ mathbf{x}}_{S}^{i} = sum_{j=1}^{n_{T}}(n_{S} pi_{ij}) mathbf{x}_{T}^{j}.$$ . Calling $ alpha_{j} = n_{S} pi_{ij}$, note that $ sum_{j} alpha_{j} = 1$, and $ alpha_{j} geq 0$. This means that $ hat{ mathbf{x}}_{S}^{i}$ lies on the convex hull of samples $ mathbf{x}_{T}^{j}$ that receive mass from $ mathbf{x}_{S}^{i}$. . Now, let us take this situation to its extreme. Suppose that all $ mathbf{x}_{T}^{j}$ for which $ pi_{ij} &gt; 0$ belong to a single class $k_{j}$ that differs from $k_{i}$. This means that $ mathbf{x}_{S}^{i}$ will be transported to the wrong region of the decision boundary, thus harming adaptation performance. In other words, if $ mathbf{x}_{S}^{i}$ corresponds to a $1$, and all $ mathbf{x}_{T}^{j}$ are $8$&#39;s, $ hat{x}_{S}^{i}$ will be closer to an $8$ than to a $1$. . Transporting and Evaluating on Target Domain . Now, we get to our final step, where we measure the success of adaptation. First we need to extract features from source domain samples, then transport them to the target domain. This is done by composing the feature extractor with the mapping $T_{ pi}$, . $$ hat{h}_{S}^{i} = T_{ pi}( phi( mathbf{x}_{S}^{i}))$$ . THs, Ys = [], [] for xs, ys in tqdm(src_loader): with torch.no_grad(): hs, _ = model(xs) hs = torch.from_numpy(otda.transform(hs.numpy())) THs.append(hs) Ys.append(ys) THs = torch.cat(THs, dim=0) Ys = torch.cat(Ys, dim=0) . 100%|██████████| 235/235 [01:15&lt;00:00, 3.11it/s] . In order to avoid overflowing the memory, we calculate the distance of mini-batches of target domain samples to the transported features, . Yp = torch.zeros_like(Yt) num_batches = len(Ht) // 64 + 1 for i in tqdm(range(num_batches), total=num_batches): ht = Ht[i * 64: (i + 1) * 64] C = torch.cdist(THs, ht, 2) ** 2 ind_opt = C.argmin(dim=0) Yp[i * 64: (i + 1) * 64] = Ys[ind_opt] . 100%|██████████| 114/114 [00:13&lt;00:00, 8.41it/s] . print(accuracy_score(Yt, Yp)) . 0.7860375805787958 . That is, we gained $7 %$ in terms of performance, which represents a gain of roughly $10 %$ in accuracy. . Want to Know More? . Optimal Transport has made a significant impact on the machine learning community in past 10 years. If you are interested in more resources and papers on the interplay between OT and Transfer Learning, you can consider looking on, . How to induce specific structures (e.g. classes) in OT maps [1, 6] | Solving issues w/ extending barycentric mappings [6, 10] | OTDA on joint distributions [11, 12] | Multi-Source Domain Adaptation [13, 14] | . References . [1] N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy, “Optimal transport for domain adaptation,” IEEE transactions on pattern analysis and machine intelligence, vol. 39, no. 9, pp. 1853–1865, 2016. . [2] Peyré, G., &amp; Cuturi, M. (2019). Computational optimal transport: With applications to data science. Foundations and Trends® in Machine Learning, 11(5-6), 355-607. . [3] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,” Neural computation, vol. 1, no. 4, pp. 541–551, 1989. . [4] J. J. Hull, “A database for handwritten text recognition research,” IEEE Transactions on pattern analysis and machine intelligence, vol. 16, no. 5, pp. 550–554, 1994 . [5] Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., ... &amp; Lempitsky, V. (2016). Domain-adversarial training of neural networks. The journal of machine learning research, 17(1), 2096-2030. . [6] Seguy, V., Damodaran, B. B., Flamary, R., Courty, N., Rolet, A., &amp; Blondel, M. (2017). Large-scale optimal transport and mapping estimation. arXiv preprint arXiv:1711.02283. . [7] Villani, C. (2009). Optimal transport: old and new (Vol. 338, p. 23). Berlin: springer. . [8] Cuturi, M. (2013). Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26. . [9] Ferradans, S., Papadakis, N., Peyré, G., &amp; Aujol, J. F. (2014). Regularized discrete optimal transport. SIAM Journal on Imaging Sciences, 7(3), 1853-1882. . [10] Perrot, M., Courty, N., Flamary, R., &amp; Habrard, A. (2016). Mapping estimation for discrete optimal transport. Advances in Neural Information Processing Systems, 29. . [11] Courty, N., Flamary, R., Habrard, A., &amp; Rakotomamonjy, A. (2017). Joint distribution optimal transportation for domain adaptation. Advances in Neural Information Processing Systems, 30. . [12] Damodaran, B. B., Kellenberger, B., Flamary, R., Tuia, D., &amp; Courty, N. (2018). Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 447-463). . [13] Nguyen, T., Le, T., Zhao, H., Tran, Q. H., Nguyen, T., &amp; Phung, D. (2021, December). Most: Multi-source domain adaptation via optimal transport for student-teacher learning. In Uncertainty in Artificial Intelligence (pp. 225-235). PMLR. . [14] Montesuma, E. F., &amp; Mboula, F. M. N. (2021). Wasserstein Barycenter for Multi-Source Domain Adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16785-16793). .",
            "url": "https://eddardd.github.io/my-personal-blog/optimal%20transport/transfer%20learning/image%20classification/2022/05/04/Optimal-Transport-for-Transfer-Learning.html",
            "relUrl": "/optimal%20transport/transfer%20learning/image%20classification/2022/05/04/Optimal-Transport-for-Transfer-Learning.html",
            "date": " • May 4, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Deep Learning com Tensorflow - Aula 2",
            "content": ". import numpy as np import tensorflow as tf import matplotlib.pyplot as plt from sklearn.manifold import TSNE from skimage.transform import rotate from sklearn.metrics import accuracy_score from sklearn.preprocessing import OneHotEncoder # Estética dos plots plt.rcParams[&#39;mathtext.fontset&#39;] = &#39;custom&#39; plt.rcParams[&#39;mathtext.rm&#39;] = &#39;Bitstream Vera Sans&#39; plt.rcParams[&#39;mathtext.it&#39;] = &#39;Bitstream Vera Sans:italic&#39; plt.rcParams[&#39;mathtext.bf&#39;] = &#39;Bitstream Vera Sans:bold&#39; plt.rcParams[&#39;font.size&#39;] = 16 plt.rcParams[&#39;mathtext.fontset&#39;] = &#39;stix&#39; plt.rcParams[&#39;font.family&#39;] = &#39;STIXGeneral&#39; . Preprocessamento dos dados . Começamos por pre-processar os dados. Para tanto, utilisaremos o próprio Tensorflow para fazer o carregamento dos dados. À partir da biblioteca Keras, carregamos os dados de treino e de teste usando a chamada . tf.keras.datasets.mnist.load_data() . Na verdade, o Tensorflow possui vários datasets comuns em machine learning. Uma lista completa pode ser encontrada no seguinte link . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() . Note que dividimos os dados em 4 arrays. Estes arrays correspondem aos dados de treino e teste. Os dados de treino correspondem àqueles que serão utilizados durante a otimização do modelo, e o de teste será usado para avaliar o modelo. Fazemos essa divisão por dois motivos: . Queremos simular a situação em que o nosso modelo é treinado num conjunto de dados fixo, e depois é utilisado na prática com dados novos, os quais o modelo não viu durante a fase de treino. Para o caso do MNIST, imagine que treinamos a rede numa base de dados local, e utilisamos o modelo para a predição em tempo real de dígitos numa aplicação remota. Os dados obtidos em tempo real não foram vistos pela rede neural durante treinamento. | As estatísticas obtidas com os dados de treinamento são geralmente mais otimistas do que em dados não vistos. Imagine o caso em que uma pessoa estuda para uma prova à partir de uma lista de exercícios. Quem você acha que teria o melhor desempenho? (1) um aluno que faz uma prova com as questões retiradas da lista, ou (2) um aluno que faz uma prova com questões inteiramente novas? | Além de dividir os dados em treino/teste, iremos também dividí-los entre características (array X) e rótulos (array y). . Formata&#231;&#227;o dos dados . Iremos começar analizando os dados como vieram no dataset da biblioteca tensorflow. Dado que as aplicações são, via de regra, para redes neurais convolucionais, os dados vem como matrizes. . Visualiza&#231;&#227;o imagens como matrizes . fig, ax = plt.subplots() ax.imshow(x_train[0], cmap=&#39;gray&#39;) _ = ax.set_xticks([]) _ = ax.set_yticks([]) print(&quot;Formato da matriz de dados: {}&quot;.format(x_train.shape)) . Formato da matriz de dados: (60000, 28, 28) . note que os dados estão salvos como imagens. Portanto, a faixa de valores para seus pixels está entre 0 e 255. Além disso, os rótulos estão salvos em formato categórico, ou seja, $y_{i} in {1, cdots, K }$, onde $K$ é o número de classes. Particularmente, $K = 10$ para o dataset MNIST. . Para converter a matriz de caracteríticas, tomaremos 2 passos: . converter de int para float, | converter da faixa [0, 255] para [0, 1] | Note que podemos aplicar a seguinte transformação, . $$ x leftarrow dfrac{x - x_{min}}{x_{max}-x_{min}}, $$Como discutido anteriormente, $x_{min} = 0$ e $x_{max} = 255$, portanto, . $$ x leftarrow dfrac{x}{255} $$Como nesse tutorial usaremos redes neurais convolucionais, as amostras permanecerão em formato de imagem. Dessa forma, temos tensores de treino/teste com formato $(N, H, W)$, onde $N_{tr} = 60000$, $H=28$ e $W=28$ . Exercício 1: Se um float ocupa 32 bits em memória, qual o espaço ocupado pelo tensor de treino? . Xtr = x_train.astype(float)[..., np.newaxis] / 255.0 Xts = x_test.astype(float)[..., np.newaxis] / 255.0 print(&quot;Formato da matriz de dados: {}&quot;.format(Xtr.shape)) print(&quot;Nova faixa de valores de X: [{}, {}]&quot;.format(Xtr.min(), Xtr.max())) . Formato da matriz de dados: (60000, 28, 28, 1) Nova faixa de valores de X: [0.0, 1.0] . Iremos também transformar a notação categórica dos rótulos na notação One Hot. Isso é simples utilizando a biblioteca scikit-learn do Python, através da classe OneHotEncoder. O exemplo abaixo fornece uma ilustração para 3 classes e 3 amostras: . $$ y^{cat} = [1, 2, 3] iff y^{OneHot} = begin{bmatrix} 1 &amp; 0 &amp; 0 0 &amp; 1 &amp; 0 0 &amp; 0 &amp; 1 end{bmatrix} $$ # OBS1: o objeto OneHotEncoder espera um array de 2 dimensões. # Porém y_train só possui 1 dimensão (observe os prints # abaixo). Para convertê-lo num array 2D, utilisaremos a # função reshape, que muda o formato do array. # OBS2: .reshape(-1, ...) faz com que a biblioteca numpy faça # uma inferência do valor adequado para a dimensão especificada # como -1. No caso, como utilisamos .reshape(-1, 1), teremos uma # transformação de formatação (N, ) -&gt; (N, 1) print(&quot;Formato de y_train antes de usar .reshape: {}&quot;.format(y_train.shape)) print(&quot;Formato de y_train após usar .reshape: {}&quot;.format(y_train.reshape(-1, 1).shape)) enc = OneHotEncoder(sparse=False) ytr = enc.fit_transform(y_train.reshape(-1, 1)) yts = enc.fit_transform(y_test.reshape(-1, 1)) print(&quot;Formato da matriz de rótulos após a aplicação da nova codificação: {}&quot;.format(ytr.shape)) . Formato de y_train antes de usar .reshape: (60000,) Formato de y_train após usar .reshape: (60000, 1) Formato da matriz de rótulos após a aplicação da nova codificação: (60000, 10) . Defini&#231;&#227;o dos modelos . def network1(input_shape=(28, 28, 1), n_classes=10): x = tf.keras.layers.Input(shape=input_shape) # Convolutional block: Convolution -&gt; Activation -&gt; Pooling y = tf.keras.layers.Conv2D(filters=36, kernel_size=(3, 3), padding=&#39;same&#39;, activation=&#39;relu&#39;)(x) y = tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding=&#39;same&#39;)(y) y = tf.keras.layers.Flatten()(y) y = tf.keras.layers.Dense(units=100, activation=&#39;relu&#39;)(y) y = tf.keras.layers.Dense(units=n_classes, activation=&#39;softmax&#39;)(y) return tf.keras.models.Model(x, y) . def network2(input_shape=(28, 28, 1), n_classes=10): x = tf.keras.layers.Input(shape=input_shape) # Convolutional block: Convolution -&gt; Activation -&gt; Pooling y = tf.keras.layers.Conv2D(filters=36, kernel_size=(7, 7), padding=&#39;same&#39;, kernel_regularizer=tf.keras.regularizers.l2(1e-3), activation=&#39;relu&#39;)(x) y = tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding=&#39;same&#39;)(y) y = tf.keras.layers.Flatten()(y) y = tf.keras.layers.Dense(units=100, activation=&#39;relu&#39;, kernel_regularizer=tf.keras.regularizers.l2(1e-3))(y) y = tf.keras.layers.Dense(units=n_classes, activation=&#39;softmax&#39;)(y) return tf.keras.models.Model(x, y) . Treinamento do modelo n&#227;o-regularizado . model1 = network1() model1.summary() . Model: &#34;functional_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 28, 28, 1)] 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 28, 28, 36) 360 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 14, 14, 36) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 7056) 0 _________________________________________________________________ dense_2 (Dense) (None, 100) 705700 _________________________________________________________________ dense_3 (Dense) (None, 10) 1010 ================================================================= Total params: 707,070 Trainable params: 707,070 Non-trainable params: 0 _________________________________________________________________ . loss_obj = tf.keras.losses.CategoricalCrossentropy() optimizer_obj = tf.keras.optimizers.Adam(lr=0.01) model1.compile(loss=loss_obj, optimizer=optimizer_obj, metrics=[&#39;accuracy&#39;]) . hist1 = model1.fit(x=Xtr, y=ytr, batch_size=1024, epochs=30, validation_data=(Xts, yts), validation_batch_size=128) . Epoch 1/30 59/59 [==============================] - 1s 21ms/step - loss: 0.3896 - accuracy: 0.8817 - val_loss: 0.0785 - val_accuracy: 0.9752 Epoch 2/30 59/59 [==============================] - 1s 18ms/step - loss: 0.0661 - accuracy: 0.9799 - val_loss: 0.0575 - val_accuracy: 0.9812 Epoch 3/30 59/59 [==============================] - 1s 18ms/step - loss: 0.0408 - accuracy: 0.9878 - val_loss: 0.0549 - val_accuracy: 0.9833 Epoch 4/30 59/59 [==============================] - 1s 18ms/step - loss: 0.0271 - accuracy: 0.9920 - val_loss: 0.0477 - val_accuracy: 0.9852 Epoch 5/30 59/59 [==============================] - 1s 18ms/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.0555 - val_accuracy: 0.9835 Epoch 6/30 59/59 [==============================] - 1s 18ms/step - loss: 0.0173 - accuracy: 0.9942 - val_loss: 0.0529 - val_accuracy: 0.9841 Epoch 7/30 29/59 [=============&gt;................] - ETA: 0s - loss: 0.0095 - accuracy: 0.9974 . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-23-8fc2a4d20239&gt; in &lt;module&gt;() -&gt; 1 hist1 = model1.fit(x=Xtr, y=ytr, batch_size=1024, epochs=30, validation_data=(Xts, yts), validation_batch_size=128) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs) 106 def _method_wrapper(self, *args, **kwargs): 107 if not self._in_multi_worker_mode(): # pylint: disable=protected-access --&gt; 108 return method(self, *args, **kwargs) 109 110 # Running inside `run_distribute_coordinator` already. /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing) 1101 logs = tmp_logs # No error, now safe to assign to logs. 1102 end_step = step + data_handler.step_increment -&gt; 1103 callbacks.on_train_batch_end(end_step, logs) 1104 epoch_logs = copy.copy(logs) 1105 /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs) 438 &#34;&#34;&#34; 439 if self._should_call_train_batch_hooks: --&gt; 440 self._call_batch_hook(ModeKeys.TRAIN, &#39;end&#39;, batch, logs=logs) 441 442 def on_test_batch_begin(self, batch, logs=None): /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook(self, mode, hook, batch, logs) 287 self._call_batch_begin_hook(mode, batch, logs) 288 elif hook == &#39;end&#39;: --&gt; 289 self._call_batch_end_hook(mode, batch, logs) 290 else: 291 raise ValueError(&#39;Unrecognized hook: {}&#39;.format(hook)) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_end_hook(self, mode, batch, logs) 307 batch_time = time.time() - self._batch_start_time 308 --&gt; 309 self._call_batch_hook_helper(hook_name, batch, logs) 310 311 if self._check_timing: /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook_helper(self, hook_name, batch, logs) 340 hook = getattr(callback, hook_name) 341 if getattr(callback, &#39;_supports_tf_logs&#39;, False): --&gt; 342 hook(batch, logs) 343 else: 344 if numpy_logs is None: # Only convert once. /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs) 959 960 def on_train_batch_end(self, batch, logs=None): --&gt; 961 self._batch_update_progbar(batch, logs) 962 963 def on_test_batch_end(self, batch, logs=None): /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _batch_update_progbar(self, batch, logs) 1014 if self.verbose == 1: 1015 # Only block async when verbose = 1. -&gt; 1016 logs = tf_utils.to_numpy_or_python_type(logs) 1017 self.progbar.update(self.seen, list(logs.items()), finalize=False) 1018 /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in to_numpy_or_python_type(tensors) 535 return t # Don&#39;t turn ragged or sparse tensors to NumPy. 536 --&gt; 537 return nest.map_structure(_to_single_numpy_or_python_type, tensors) 538 539 /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs) 633 634 return pack_sequence_as( --&gt; 635 structure[0], [func(*x) for x in entries], 636 expand_composites=expand_composites) 637 /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in &lt;listcomp&gt;(.0) 633 634 return pack_sequence_as( --&gt; 635 structure[0], [func(*x) for x in entries], 636 expand_composites=expand_composites) 637 /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in _to_single_numpy_or_python_type(t) 531 def _to_single_numpy_or_python_type(t): 532 if isinstance(t, ops.Tensor): --&gt; 533 x = t.numpy() 534 return x.item() if np.ndim(x) == 0 else x 535 return t # Don&#39;t turn ragged or sparse tensors to NumPy. /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self) 1061 &#34;&#34;&#34; 1062 # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors. -&gt; 1063 maybe_arr = self._numpy() # pylint: disable=protected-access 1064 return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr 1065 /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self) 1027 def _numpy(self): 1028 try: -&gt; 1029 return self._numpy_internal() 1030 except core._NotOkStatusException as e: # pylint: disable=protected-access 1031 six.raise_from(core._status_to_exception(e.code, e.message), None) # pylint: disable=protected-access KeyboardInterrupt: . . fig, axes = plt.subplots(1, 2, figsize=(15, 5)) axes[0].plot(100 * np.array(hist1.history[&#39;accuracy&#39;]), label=&#39;Treino&#39;) axes[0].plot(100 * np.array(hist1.history[&#39;val_accuracy&#39;]), label=&#39;Teste&#39;) axes[0].set_ylabel(&#39;Percentual de Acerto&#39;) axes[0].set_xlabel(&#39;Época&#39;) axes[0].legend() axes[1].plot(100 * np.array(hist1.history[&#39;loss&#39;]), label=&#39;Treino&#39;) axes[1].plot(100 * np.array(hist1.history[&#39;val_loss&#39;]), label=&#39;Teste&#39;) axes[1].set_ylabel(&#39;Função de Erro&#39;) axes[1].set_xlabel(&#39;Época&#39;) axes[1].legend() . &lt;matplotlib.legend.Legend at 0x7ff4fddc3128&gt; . filters = model1.weights[0].numpy() fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(filters[:, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) . inp = model1.layers[0].input outs = [layer.output for layer in model1.layers] layerized_model1 = tf.keras.models.Model(inp, outs) Omat = layerized_model1.predict(Xts[0, ...].reshape(1, 28, 28, 1)) . plt.imshow(Omat[0][0, :, :, 0], cmap=&#39;gray&#39;) plt.yticks([]) plt.xticks([]) . ([], &lt;a list of 0 Text major ticklabel objects&gt;) . fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(Omat[1][0, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(Omat[2][0, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) . fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(x, cmap=&#39;gray&#39;) axes[0].set_xticks([]) axes[0].set_yticks([]) axes[1].bar(np.arange(10), Omat[-1][0, :]) _ = axes[1].set_xticks([i for i in range(10)]) . x = Omat[0][0, :, :, 0] xrot = rotate(x, angle=15) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(x, cmap=&#39;gray&#39;) axes[0].set_xticks([]) axes[0].set_yticks([]) axes[1].imshow(xrot, cmap=&#39;gray&#39;) axes[1].set_xticks([]) axes[1].set_yticks([]) . [] . tmp = layerized_model.predict(xrot.reshape(1, 28, 28, 1)) fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(tmp[1][0, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(tmp[2][0, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) . fig, axes = plt.subplots(2, 2, figsize=(10, 5)) axes[0, 0].imshow(x, cmap=&#39;gray&#39;) axes[0, 0].set_xticks([]) axes[0, 0].set_yticks([]) axes[0, 1].imshow(xrot, cmap=&#39;gray&#39;) axes[0, 1].set_xticks([]) axes[0, 1].set_yticks([]) axes[1, 0].bar(np.arange(10), Omat[-1][0, :]) axes[1, 1].bar(np.arange(10), tmp[-1][0, :]) . &lt;BarContainer object of 10 artists&gt; . x = Omat[0][0, :, :, 0] xrot = rotate(x, angle=45) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(x, cmap=&#39;gray&#39;) axes[0].set_xticks([]) axes[0].set_yticks([]) axes[1].imshow(xrot, cmap=&#39;gray&#39;) axes[1].set_xticks([]) axes[1].set_yticks([]) . [] . tmp = layerized_model1.predict(xrot.reshape(1, 28, 28, 1)) fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(tmp[1][0, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(tmp[2][0, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) . fig, axes = plt.subplots(2, 2, figsize=(10, 5)) axes[0, 0].imshow(x, cmap=&#39;gray&#39;) axes[0, 0].set_xticks([]) axes[0, 0].set_yticks([]) axes[0, 1].imshow(xrot, cmap=&#39;gray&#39;) axes[0, 1].set_xticks([]) axes[0, 1].set_yticks([]) axes[1, 0].bar(np.arange(10), Omat[-1][0, :]) axes[1, 0].set_ylim([0, 1.1]) _ = axes[1, 0].set_xticks([i for i in range(0, 10)]) axes[1, 1].bar(np.arange(10), tmp[-1][0, :]) axes[1, 1].set_ylim([0, 1.1]) _ = axes[1, 1].set_xticks([i for i in range(0, 10)]) . x = Omat[0][0, :, :, 0] noise = 0.5 * np.random.randn(*x.shape) xnoise = np.clip(x + noise, 0.0, 1.0) fig, axes = plt.subplots(1, 3, figsize=(10, 5)) axes[0].imshow(x, cmap=&#39;gray&#39;) axes[0].set_xticks([]) axes[0].set_yticks([]) axes[1].imshow(xnoise, cmap=&#39;gray&#39;) axes[1].set_xticks([]) axes[1].set_yticks([]) axes[2].imshow(noise, cmap=&#39;gray&#39;) axes[2].set_xticks([]) axes[2].set_yticks([]) . [] . tmp = layerized_model1.predict(xnoise.reshape(1, 28, 28, 1)) . fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(tmp[1][0, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(tmp[2][0, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) . fig, axes = plt.subplots(2, 2, figsize=(10, 5)) axes[0, 0].imshow(x, cmap=&#39;gray&#39;) axes[0, 0].set_xticks([]) axes[0, 0].set_yticks([]) axes[0, 1].imshow(xnoise, cmap=&#39;gray&#39;) axes[0, 1].set_xticks([]) axes[0, 1].set_yticks([]) axes[1, 0].bar(np.arange(10), Omat[-1][0, :]) axes[1, 0].set_ylim([0, 1.1]) _ = axes[1, 0].set_xticks([i for i in range(0, 10)]) axes[1, 1].bar(np.arange(10), tmp[-1][0, :]) axes[1, 1].set_ylim([0, 1.1]) _ = axes[1, 1].set_xticks([i for i in range(0, 10)]) . y = Xts[1, ...].reshape(28, 28) alpha = 0.25 interpol_xy = np.clip((1 - alpha) * x + alpha * y, 0.0, 1.0) fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(x, cmap=&#39;gray&#39;) axes[0].set_xticks([]) axes[0].set_yticks([]) axes[1].imshow(interpol_xy, cmap=&#39;gray&#39;) axes[1].set_xticks([]) axes[1].set_yticks([]) . [] . tmp = layerized_model1.predict(interpol_xy.reshape(1, 28, 28, 1)) fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(tmp[1][0, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(tmp[2][0, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) . fig, axes = plt.subplots(2, 2, figsize=(10, 5)) axes[0, 0].imshow(x, cmap=&#39;gray&#39;) axes[0, 0].set_xticks([]) axes[0, 0].set_yticks([]) axes[0, 1].imshow(interpol_xy, cmap=&#39;gray&#39;) axes[0, 1].set_xticks([]) axes[0, 1].set_yticks([]) axes[1, 0].bar(np.arange(10), Omat[-1][0, :]) axes[1, 0].set_ylim([0, 1.1]) _ = axes[1, 0].set_xticks([i for i in range(0, 10)]) axes[1, 1].bar(np.arange(10), tmp[-1][0, :]) axes[1, 1].set_ylim([0, 1.1]) _ = axes[1, 1].set_xticks([i for i in range(0, 10)]) . Treinamento do modelo regularizado . model2 = network2() model2.summary() . Model: &#34;functional_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 28, 28, 1)] 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 28, 28, 36) 1800 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 14, 14, 36) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 7056) 0 _________________________________________________________________ dense_2 (Dense) (None, 100) 705700 _________________________________________________________________ dense_3 (Dense) (None, 10) 1010 ================================================================= Total params: 708,510 Trainable params: 708,510 Non-trainable params: 0 _________________________________________________________________ . loss_obj = tf.keras.losses.CategoricalCrossentropy() optimizer_obj = tf.keras.optimizers.Adam(lr=0.01) model2.compile(loss=loss_obj, optimizer=optimizer_obj, metrics=[&#39;accuracy&#39;]) . hist2 = model2.fit(x=Xtr, y=ytr, batch_size=1024, epochs=30, validation_data=(Xts, yts), validation_batch_size=128) . Epoch 1/30 59/59 [==============================] - 1s 22ms/step - loss: 0.6784 - accuracy: 0.8953 - val_loss: 0.2166 - val_accuracy: 0.9735 Epoch 2/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1922 - accuracy: 0.9717 - val_loss: 0.1650 - val_accuracy: 0.9763 Epoch 3/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1672 - accuracy: 0.9746 - val_loss: 0.1539 - val_accuracy: 0.9772 Epoch 4/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1580 - accuracy: 0.9770 - val_loss: 0.1367 - val_accuracy: 0.9819 Epoch 5/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1517 - accuracy: 0.9775 - val_loss: 0.1408 - val_accuracy: 0.9799 Epoch 6/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1460 - accuracy: 0.9791 - val_loss: 0.1364 - val_accuracy: 0.9827 Epoch 7/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1401 - accuracy: 0.9804 - val_loss: 0.1364 - val_accuracy: 0.9778 Epoch 8/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1428 - accuracy: 0.9794 - val_loss: 0.1384 - val_accuracy: 0.9777 Epoch 9/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1334 - accuracy: 0.9815 - val_loss: 0.1343 - val_accuracy: 0.9805 Epoch 10/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1312 - accuracy: 0.9815 - val_loss: 0.1217 - val_accuracy: 0.9823 Epoch 11/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1380 - accuracy: 0.9801 - val_loss: 0.1439 - val_accuracy: 0.9765 Epoch 12/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1320 - accuracy: 0.9817 - val_loss: 0.1237 - val_accuracy: 0.9818 Epoch 13/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1326 - accuracy: 0.9808 - val_loss: 0.1493 - val_accuracy: 0.9763 Epoch 14/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1283 - accuracy: 0.9819 - val_loss: 0.1145 - val_accuracy: 0.9840 Epoch 15/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1305 - accuracy: 0.9812 - val_loss: 0.1307 - val_accuracy: 0.9805 Epoch 16/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1255 - accuracy: 0.9827 - val_loss: 0.1200 - val_accuracy: 0.9832 Epoch 17/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1242 - accuracy: 0.9823 - val_loss: 0.1394 - val_accuracy: 0.9755 Epoch 18/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1242 - accuracy: 0.9825 - val_loss: 0.1146 - val_accuracy: 0.9857 Epoch 19/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1223 - accuracy: 0.9827 - val_loss: 0.1157 - val_accuracy: 0.9829 Epoch 20/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1233 - accuracy: 0.9825 - val_loss: 0.1206 - val_accuracy: 0.9837 Epoch 21/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1189 - accuracy: 0.9840 - val_loss: 0.1135 - val_accuracy: 0.9849 Epoch 22/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1234 - accuracy: 0.9819 - val_loss: 0.1268 - val_accuracy: 0.9810 Epoch 23/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1202 - accuracy: 0.9830 - val_loss: 0.1265 - val_accuracy: 0.9802 Epoch 24/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1194 - accuracy: 0.9830 - val_loss: 0.1245 - val_accuracy: 0.9803 Epoch 25/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1224 - accuracy: 0.9825 - val_loss: 0.1144 - val_accuracy: 0.9826 Epoch 26/30 59/59 [==============================] - 1s 18ms/step - loss: 0.1149 - accuracy: 0.9842 - val_loss: 0.1087 - val_accuracy: 0.9829 Epoch 27/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1193 - accuracy: 0.9827 - val_loss: 0.1279 - val_accuracy: 0.9803 Epoch 28/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1157 - accuracy: 0.9837 - val_loss: 0.1053 - val_accuracy: 0.9868 Epoch 29/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1167 - accuracy: 0.9826 - val_loss: 0.1192 - val_accuracy: 0.9820 Epoch 30/30 59/59 [==============================] - 1s 19ms/step - loss: 0.1134 - accuracy: 0.9840 - val_loss: 0.1102 - val_accuracy: 0.9831 . . fig, axes = plt.subplots(1, 2, figsize=(15, 5)) axes[0].plot(100 * np.array(hist2.history[&#39;accuracy&#39;]), label=&#39;Treino&#39;) axes[0].plot(100 * np.array(hist2.history[&#39;val_accuracy&#39;]), label=&#39;Teste&#39;) axes[0].set_ylabel(&#39;Percentual de Acerto&#39;) axes[0].set_xlabel(&#39;Época&#39;) axes[0].legend() axes[1].plot(100 * np.array(hist2.history[&#39;loss&#39;]), label=&#39;Treino&#39;) axes[1].plot(100 * np.array(hist2.history[&#39;val_loss&#39;]), label=&#39;Teste&#39;) axes[1].set_ylabel(&#39;Função de Erro&#39;) axes[1].set_xlabel(&#39;Época&#39;) axes[1].legend() . &lt;matplotlib.legend.Legend at 0x7ff4fd196940&gt; . filters = model2.weights[0].numpy() fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(filters[:, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) . inp = model2.layers[0].input outs = [layer.output for layer in model2.layers] layerized_model2 = tf.keras.models.Model(inp, outs) Omat = layerized_model2.predict(Xts[0, ...].reshape(1, 28, 28, 1)) . plt.imshow(Omat[0][0, :, :, 0], cmap=&#39;gray&#39;) plt.yticks([]) plt.xticks([]) . ([], &lt;a list of 0 Text major ticklabel objects&gt;) . fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(Omat[1][0, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) fig, axes = plt.subplots(6, 6, figsize=(8, 8)) for i, ax in enumerate(axes.flatten()): ax.imshow(np.squeeze(Omat[2][0, :, :, i]), cmap=&#39;gray&#39;) ax.set_yticks([]) ax.set_xticks([]) . fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(x, cmap=&#39;gray&#39;) axes[0].set_xticks([]) axes[0].set_yticks([]) axes[1].bar(np.arange(10), Omat[-1][0, :]) _ = axes[1].set_xticks([i for i in range(10)]) . Compara&#231;&#227;o entre os dois modelos . Acur&#225;cia . fig, axes = plt.subplots(2, 1, figsize=(15, 5)) axes[0].plot(hist1.history[&#39;accuracy&#39;], label=&#39;Modelo 1&#39;) axes[0].plot(hist2.history[&#39;accuracy&#39;], label=&#39;Modelo 2&#39;) axes[0].legend() axes[1].plot(hist1.history[&#39;val_accuracy&#39;], label=&#39;Modelo 1&#39;) axes[1].plot(hist2.history[&#39;val_accuracy&#39;], label=&#39;Modelo 2&#39;) axes[1].legend() . &lt;matplotlib.legend.Legend at 0x7ff4fd14d198&gt; . Custo . fig, axes = plt.subplots(2, 1, figsize=(15, 5)) axes[0].plot(hist1.history[&#39;loss&#39;], label=&#39;Modelo 1&#39;) axes[0].plot(hist2.history[&#39;loss&#39;], label=&#39;Modelo 2&#39;) axes[0].legend() axes[1].plot(hist1.history[&#39;val_loss&#39;], label=&#39;Modelo 1&#39;) axes[1].plot(hist2.history[&#39;val_loss&#39;], label=&#39;Modelo 2&#39;) axes[1].legend() . &lt;matplotlib.legend.Legend at 0x7ff4fd15be48&gt; . M&#233;tricas . yp1 = model1.predict(Xts).argmax(axis=1) yp2 = model2.predict(Xts).argmax(axis=1) . print(&quot;Taxa de precisão: {}&quot;.format(100 * accuracy_score(y_test, yp1))) print(&quot;Taxa de precisão (Regularizado): {}&quot;.format(100 * accuracy_score(y_test, yp2))) . Taxa de precisão: 98.98 Taxa de precisão (Regularizado): 98.31 . Visualiza&#231;&#227;o das representa&#231;&#245;es . Omat = layerized_model2.predict(Xts) . sample_inds = [] for i in np.unique(y_test): sample_inds.append(np.where(y_test == i)[0][:100]) sample_inds = np.concatenate(sample_inds, axis=0) . raw_rep = Omat[0][sample_inds, :] print(raw_rep.shape) tsne = TSNE(n_components=2, init=&#39;pca&#39;, verbose=True) tsne.fit(raw_rep.reshape(-1, 784)) embedding = tsne.embedding_ . (1000, 28, 28, 1) [t-SNE] Computing 91 nearest neighbors... [t-SNE] Indexed 1000 samples in 0.056s... [t-SNE] Computed neighbors for 1000 samples in 1.707s... [t-SNE] Computed conditional probabilities for sample 1000 / 1000 [t-SNE] Mean sigma: 2.611868 [t-SNE] KL divergence after 250 iterations with early exaggeration: 70.110123 [t-SNE] KL divergence after 1000 iterations: 1.031477 . fig, ax = plt.subplots(1, 1, figsize=(8, 8)) for i in np.unique(y_test): ax.scatter(embedding[100 * i: 100 * (i + 1), 0], embedding[100 * i: 100 * (i + 1), 1], label=&#39;Digit {}&#39;.format(i)) ax.legend(loc=&#39;upper right&#39;, bbox_to_anchor=(1.3, 1.0)) . &lt;matplotlib.legend.Legend at 0x7ff4fc52c6d8&gt; . conv_rep = Omat[3][sample_inds, :] print(conv_rep.shape) tsne = TSNE(n_components=2, init=&#39;pca&#39;, verbose=True) tsne.fit(conv_rep) embedding = tsne.embedding_ . [t-SNE] Computing 91 nearest neighbors... [t-SNE] Indexed 1000 samples in 0.508s... [t-SNE] Computed neighbors for 1000 samples in 15.771s... [t-SNE] Computed conditional probabilities for sample 1000 / 1000 [t-SNE] Mean sigma: 2.780777 [t-SNE] KL divergence after 250 iterations with early exaggeration: 71.441025 [t-SNE] KL divergence after 1000 iterations: 0.994165 . fig, ax = plt.subplots(1, 1, figsize=(8, 8)) for i in np.unique(y_test): ax.scatter(embedding[100 * i: 100 * (i + 1), 0], embedding[100 * i: 100 * (i + 1), 1], label=&#39;Digit {}&#39;.format(i)) ax.legend(loc=&#39;upper right&#39;, bbox_to_anchor=(1.3, 1.0)) . &lt;matplotlib.legend.Legend at 0x7ff4fdece5f8&gt; . dense_rep = Omat[-2][sample_inds, :] tsne = TSNE(n_components=2, init=&#39;pca&#39;, verbose=True) tsne.fit(dense_rep) embedding = tsne.embedding_ . [t-SNE] Computing 91 nearest neighbors... [t-SNE] Indexed 1000 samples in 0.007s... [t-SNE] Computed neighbors for 1000 samples in 0.217s... [t-SNE] Computed conditional probabilities for sample 1000 / 1000 [t-SNE] Mean sigma: 1.311268 [t-SNE] KL divergence after 250 iterations with early exaggeration: 54.055824 [t-SNE] KL divergence after 1000 iterations: 0.571683 . fig, ax = plt.subplots(1, 1, figsize=(8, 8)) for i in np.unique(y_test): ax.scatter(embedding[100 * i: 100 * (i + 1), 0], embedding[100 * i: 100 * (i + 1), 1], label=&#39;Digit {}&#39;.format(i)) ax.legend(loc=&#39;upper right&#39;, bbox_to_anchor=(1.3, 1.0)) . &lt;matplotlib.legend.Legend at 0x7ff4fdea9c88&gt; .",
            "url": "https://eddardd.github.io/my-personal-blog/deep%20learning/tensorflow/image%20classification/neural%20networks/2020/12/01/Deep-Learning-com-Tensorflow-Aula2.html",
            "relUrl": "/deep%20learning/tensorflow/image%20classification/neural%20networks/2020/12/01/Deep-Learning-com-Tensorflow-Aula2.html",
            "date": " • Dec 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Deep Learning com Tensorflow - Aula 1",
            "content": ". import numpy as np import tensorflow as tf import matplotlib.pyplot as plt from sklearn.metrics import accuracy_score from sklearn.preprocessing import OneHotEncoder # Estética dos plots plt.rcParams[&#39;mathtext.fontset&#39;] = &#39;custom&#39; plt.rcParams[&#39;mathtext.rm&#39;] = &#39;Bitstream Vera Sans&#39; plt.rcParams[&#39;mathtext.it&#39;] = &#39;Bitstream Vera Sans:italic&#39; plt.rcParams[&#39;mathtext.bf&#39;] = &#39;Bitstream Vera Sans:bold&#39; plt.rcParams[&#39;font.size&#39;] = 16 plt.rcParams[&#39;mathtext.fontset&#39;] = &#39;stix&#39; plt.rcParams[&#39;font.family&#39;] = &#39;STIXGeneral&#39; . Preprocessamento dos dados . Começamos por pre-processar os dados. Para tanto, utilisaremos o próprio Tensorflow para fazer o carregamento dos dados. À partir da biblioteca Keras, carregamos os dados de treino e de teste usando a chamada . tf.keras.datasets.mnist.load_data() . Na verdade, o Tensorflow possui vários datasets comuns em machine learning. Uma lista completa pode ser encontrada no seguinte link . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step . Note que dividimos os dados em 4 arrays. Estes arrays correspondem aos dados de treino e teste. Os dados de treino correspondem àqueles que serão utilizados durante a otimização do modelo, e o de teste será usado para avaliar o modelo. Fazemos essa divisão por dois motivos: . Queremos simular a situação em que o nosso modelo é treinado num conjunto de dados fixo, e depois é utilisado na prática com dados novos, os quais o modelo não viu durante a fase de treino. Para o caso do MNIST, imagine que treinamos a rede numa base de dados local, e utilisamos o modelo para a predição em tempo real de dígitos numa aplicação remota. Os dados obtidos em tempo real não foram vistos pela rede neural durante treinamento. | As estatísticas obtidas com os dados de treinamento são geralmente mais otimistas do que em dados não vistos. Imagine o caso em que uma pessoa estuda para uma prova à partir de uma lista de exercícios. Quem você acha que teria o melhor desempenho? (1) um aluno que faz uma prova com as questões retiradas da lista, ou (2) um aluno que faz uma prova com questões inteiramente novas? | Além de dividir os dados em treino/teste, iremos também dividí-los entre características (array X) e rótulos (array y). . Formata&#231;&#227;o dos dados . Iremos começar analizando os dados como vieram no dataset da biblioteca tensorflow. Dado que as aplicações são, via de regra, para redes neurais convolucionais, os dados vem como matrizes. . Visualiza&#231;&#227;o imagens como matrizes . fig, ax = plt.subplots() ax.imshow(x_train[0], cmap=&#39;gray&#39;) _ = ax.set_xticks([]) _ = ax.set_yticks([]) print(&quot;Formato da matriz de dados: {}&quot;.format(x_train.shape)) . Formato da matriz de dados: (60000, 28, 28) . note que os dados estão salvos como imagens. Portanto, a faixa de valores para seus pixels está entre 0 e 255. Além disso, os rótulos estão salvos em formato categórico, ou seja, $y_{i} in {1, cdots, K }$, onde $K$ é o número de classes. Particularmente, $K = 10$ para o dataset MNIST. . print(&quot;Faixa de valores de X: [{}, {}]&quot;.format(x_train.min(), x_train.max())) print(&quot;Tipos de dados das matrizes: X {}, y {}&quot;.format(x_train.dtype, x_test.dtype)) print(&quot;Codificação dos rótulos: {}&quot;.format(y_train[0])) . Faixa de valores de X: [0, 255] Tipos de dados das matrizes: X uint8, y uint8 Codificação dos rótulos: 5 . Para converter a matriz de caracteríticas, tomaremos 2 passos: . converter de int para float, | converter da faixa [0, 255] para [0, 1] | Note que podemos aplicar a seguinte transformação, . $$ x leftarrow dfrac{x - x_{min}}{x_{max}-x_{min}}, $$Como discutido anteriormente, $x_{min} = 0$ e $x_{max} = 255$, portanto, . $$ x leftarrow dfrac{x}{255} $$ Xtr = x_train.astype(float) / 255.0 Xts = x_test.astype(float) / 255.0 print(&quot;Nova faixa de valores de X: [{}, {}]&quot;.format(Xtr.min(), Xtr.max())) . Nova faixa de valores de X: [0.0, 1.0] . Precisamos ainda transformar o formato dos dados. Para tanto, queremos converter cada imagem-matriz em imagem-vetor através da notação Row-Major. Isso é particularmente simples em Python. Utilizamos o método $.reshape$ da classe $ndarray$, . # OBS: o uso de -1 numa das dimensões do reshape faz com que numpy infira o valor # da dada dimensão. Xtr = Xtr.reshape(-1, 28 * 28) Xts = Xts.reshape(-1, 28 * 28) print(&quot;Novo formato de X: {}&quot;.format(Xtr.shape)) . Novo formato de X: (60000, 784) . Além disso, iremos também transformar a notação categórica dos rótulos na notação One Hot. Isso é simples utilizando a biblioteca scikit-learn do Python, através da classe OneHotEncoder. O exemplo abaixo fornece uma ilustração para 3 classes e 3 amostras: . $$ y^{cat} = [1, 2, 3] iff y^{OneHot} = begin{bmatrix} 1 &amp; 0 &amp; 0 0 &amp; 1 &amp; 0 0 &amp; 0 &amp; 1 end{bmatrix} $$ # OBS1: o objeto OneHotEncoder espera um array de 2 dimensões. # Porém y_train só possui 1 dimensão (observe os prints # abaixo). Para convertê-lo num array 2D, utilisaremos a # função reshape, que muda o formato do array. # OBS2: .reshape(-1, ...) faz com que a biblioteca numpy faça # uma inferência do valor adequado para a dimensão especificada # como -1. No caso, como utilisamos .reshape(-1, 1), teremos uma # transformação de formatação (N, ) -&gt; (N, 1) print(&quot;Formato de y_train antes de usar .reshape: {}&quot;.format(y_train.shape)) print(&quot;Formato de y_train após usar .reshape: {}&quot;.format(y_train.reshape(-1, 1).shape)) enc = OneHotEncoder(sparse=False) ytr = enc.fit_transform(y_train.reshape(-1, 1)) yts = enc.fit_transform(y_test.reshape(-1, 1)) print(&quot;Formato da matriz de rótulos após a aplicação da nova codificação: {}&quot;.format(ytr.shape)) . Formato de y_train antes de usar .reshape: (60000,) Formato de y_train após usar .reshape: (60000, 1) Formato da matriz de rótulos após a aplicação da nova codificação: (60000, 10) . Treinamento Perceptron Simples . Aqui treinaremos uma rede Perceptron simples com uma camada. Para tanto, utilisaremos as seguintes classes da biblioteca Keras, . Keras Layers Input Layer | Dense Layer | . | Keras Model | . Defini&#231;&#227;o da rede . Aqui, temos apenas 2 camadas: a camada de entrada, que recebe uma matriz $(N, d)$, onde $N$ é o número de amostras e $d$ é o número de características. . Temos uma segunda camada, chamada de camada de output, que toma como entrada o o objeto simbólico de ouptut da camada de entrada, e tem como saída o objeto simbólico . $$ mathbf{y} = varphi( mathbf{Wx} + mathbf{b}). $$Iremos portanto por estes conceitos dentro de uma função, que irá ter como saída um objeto $tf.keras.models.Model$. . def perceptron_mnist(input_shape=(784,), n_classes=10): x = tf.keras.layers.Input(shape=input_shape) y = tf.keras.layers.Dense(units=n_classes, activation=&#39;sigmoid&#39;)(x) return tf.keras.models.Model(x, y) . model1 = perceptron_mnist() # Print do modelo construído model1.summary() . Model: &#34;functional_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 784)] 0 _________________________________________________________________ dense (Dense) (None, 10) 7850 ================================================================= Total params: 7,850 Trainable params: 7,850 Non-trainable params: 0 _________________________________________________________________ . Compila&#231;&#227;o do modelo . Para compilar o modelo, precisaremos definir: . Funções de Custo | Otimizadores | em especial, iremos utilisar o erro quadrático médio, definido por, . $$ mathcal{L}( mathbf{W}, mathbf{b}) = dfrac{1}{2N}|| mathbf{y} - hat{ mathbf{y}}||_{2}^{2}, mathcal{L}( mathbf{W}, mathbf{b}) = dfrac{1}{2N} sum_{i=1}^{N}( mathbf{y}_{i} - varphi( mathbf{Wx}_{i} + mathbf{b}))^{2} $$e definiremos o otimizador Stochastic Gradient Descent (SGD), que atualizará os parâmetros da rede neural através da regra: . $$ mathbf{W}^{ ell + 1} leftarrow mathbf{W}^{ ell} - eta dfrac{ partial mathcal{L}}{ partial mathbf{W}}, mathbf{b}^{ ell + 1} leftarrow mathbf{b}^{ ell} - eta dfrac{ partial mathcal{L}}{ partial mathbf{b}}. $$onde $ eta$ é um parâmetro escolhido previamente, que define quão longo é o passo de otimização tomado na direção do gradiente. No contexto de machine learning, $ eta$ é chamado de Learning Rate. . O que faz a compilação do modelo? A compilação de um modelo Keras faz o seguinte: . Para cada operação no grafo computacional (construído anteriormente pela função que define o modelo), calcula os gradientes. | Define a regra para a atualização dos parâmetros | Inicializa cada variável no modelo. | Basicamente a compilação prepara o modelo para duas tarefas: inferência (feed-forward) e aprendizado (backpropagation). . # Passo à passo: # 1. Instancie a função de custo (num primeiro momento, use MeanSquaredError) # 2. Instancie o otimizador (SGD, ou Stochastic Gradient Descent) # 3. Compile o modelo. # 1. Instanciação do custo loss_obj = tf.keras.losses.MeanSquaredError() # 2. Instanciação do otimizador optimizer_obj = tf.keras.optimizers.SGD(learning_rate=1e-1) # 3. Compilação do modelo model1.compile( loss=loss_obj, optimizer=optimizer_obj, metrics=[&#39;accuracy&#39;] ) . Uma vez que o modelo foi compilado, podemos lançar seu aprendizado. fazemos isso com a função .fit. Em especial, definiremos, . A matriz de treino (caracteríticas), x, que em nossa notação é Xtr, | A matriz de treino (rótulos), y, que em nossa notação é ytr, | O tamanho dos minibatches, _batch_size_, que definiremos como 1024, | O número de épocas, _n_epochs_, que definiremos como 150, | Os dados de validação, _validation_data_, que na nossa notação é a dupla $(Xtr, ytr)$, | O _batch_size_ dos dados de validação, que utilisaremos 128. | Podemos ainda salvar o histórico de treinamento, que contém um dicionário com várias métricas por época de treinamento. . hist1 = model1.fit(x=Xtr, y=ytr, batch_size=1024, epochs=150, validation_data=(Xts, yts), validation_batch_size=128) . Epoch 1/150 59/59 [==============================] - 1s 9ms/step - loss: 0.1384 - accuracy: 0.1834 - val_loss: 0.0972 - val_accuracy: 0.2847 Epoch 2/150 59/59 [==============================] - 0s 8ms/step - loss: 0.0923 - accuracy: 0.3376 - val_loss: 0.0872 - val_accuracy: 0.4022 Epoch 3/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0854 - accuracy: 0.4326 - val_loss: 0.0825 - val_accuracy: 0.4715 Epoch 4/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0811 - accuracy: 0.4882 - val_loss: 0.0786 - val_accuracy: 0.5175 Epoch 5/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0775 - accuracy: 0.5295 - val_loss: 0.0751 - val_accuracy: 0.5493 Epoch 6/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0741 - accuracy: 0.5622 - val_loss: 0.0719 - val_accuracy: 0.5802 Epoch 7/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0710 - accuracy: 0.5905 - val_loss: 0.0689 - val_accuracy: 0.6073 Epoch 8/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0682 - accuracy: 0.6138 - val_loss: 0.0662 - val_accuracy: 0.6313 Epoch 9/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0657 - accuracy: 0.6349 - val_loss: 0.0637 - val_accuracy: 0.6522 Epoch 10/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0634 - accuracy: 0.6527 - val_loss: 0.0616 - val_accuracy: 0.6674 Epoch 11/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0614 - accuracy: 0.6697 - val_loss: 0.0596 - val_accuracy: 0.6828 Epoch 12/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0595 - accuracy: 0.6840 - val_loss: 0.0579 - val_accuracy: 0.6962 Epoch 13/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0579 - accuracy: 0.6981 - val_loss: 0.0563 - val_accuracy: 0.7090 Epoch 14/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0564 - accuracy: 0.7113 - val_loss: 0.0548 - val_accuracy: 0.7237 Epoch 15/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0550 - accuracy: 0.7253 - val_loss: 0.0535 - val_accuracy: 0.7368 Epoch 16/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0538 - accuracy: 0.7365 - val_loss: 0.0523 - val_accuracy: 0.7493 Epoch 17/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0526 - accuracy: 0.7491 - val_loss: 0.0512 - val_accuracy: 0.7624 Epoch 18/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0515 - accuracy: 0.7612 - val_loss: 0.0501 - val_accuracy: 0.7737 Epoch 19/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0505 - accuracy: 0.7709 - val_loss: 0.0491 - val_accuracy: 0.7858 Epoch 20/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0496 - accuracy: 0.7811 - val_loss: 0.0482 - val_accuracy: 0.7955 Epoch 21/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0487 - accuracy: 0.7893 - val_loss: 0.0473 - val_accuracy: 0.8022 Epoch 22/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0478 - accuracy: 0.7966 - val_loss: 0.0465 - val_accuracy: 0.8090 Epoch 23/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0470 - accuracy: 0.8025 - val_loss: 0.0457 - val_accuracy: 0.8140 Epoch 24/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0463 - accuracy: 0.8077 - val_loss: 0.0450 - val_accuracy: 0.8197 Epoch 25/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0456 - accuracy: 0.8116 - val_loss: 0.0443 - val_accuracy: 0.8234 Epoch 26/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0449 - accuracy: 0.8154 - val_loss: 0.0437 - val_accuracy: 0.8272 Epoch 27/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0443 - accuracy: 0.8184 - val_loss: 0.0431 - val_accuracy: 0.8289 Epoch 28/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0437 - accuracy: 0.8212 - val_loss: 0.0425 - val_accuracy: 0.8323 Epoch 29/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0431 - accuracy: 0.8234 - val_loss: 0.0420 - val_accuracy: 0.8342 Epoch 30/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0426 - accuracy: 0.8256 - val_loss: 0.0414 - val_accuracy: 0.8370 Epoch 31/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0421 - accuracy: 0.8274 - val_loss: 0.0409 - val_accuracy: 0.8388 Epoch 32/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0416 - accuracy: 0.8293 - val_loss: 0.0405 - val_accuracy: 0.8403 Epoch 33/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0412 - accuracy: 0.8309 - val_loss: 0.0400 - val_accuracy: 0.8414 Epoch 34/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0407 - accuracy: 0.8322 - val_loss: 0.0396 - val_accuracy: 0.8426 Epoch 35/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0403 - accuracy: 0.8338 - val_loss: 0.0392 - val_accuracy: 0.8439 Epoch 36/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0399 - accuracy: 0.8353 - val_loss: 0.0388 - val_accuracy: 0.8449 Epoch 37/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0395 - accuracy: 0.8369 - val_loss: 0.0385 - val_accuracy: 0.8463 Epoch 38/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0392 - accuracy: 0.8385 - val_loss: 0.0381 - val_accuracy: 0.8477 Epoch 39/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0388 - accuracy: 0.8395 - val_loss: 0.0378 - val_accuracy: 0.8484 Epoch 40/150 59/59 [==============================] - 0s 8ms/step - loss: 0.0385 - accuracy: 0.8402 - val_loss: 0.0374 - val_accuracy: 0.8490 Epoch 41/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0382 - accuracy: 0.8413 - val_loss: 0.0371 - val_accuracy: 0.8500 Epoch 42/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0379 - accuracy: 0.8420 - val_loss: 0.0368 - val_accuracy: 0.8507 Epoch 43/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0376 - accuracy: 0.8427 - val_loss: 0.0365 - val_accuracy: 0.8524 Epoch 44/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0373 - accuracy: 0.8438 - val_loss: 0.0363 - val_accuracy: 0.8534 Epoch 45/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0370 - accuracy: 0.8448 - val_loss: 0.0360 - val_accuracy: 0.8545 Epoch 46/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0368 - accuracy: 0.8458 - val_loss: 0.0357 - val_accuracy: 0.8551 Epoch 47/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0365 - accuracy: 0.8464 - val_loss: 0.0355 - val_accuracy: 0.8559 Epoch 48/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0363 - accuracy: 0.8471 - val_loss: 0.0353 - val_accuracy: 0.8562 Epoch 49/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0360 - accuracy: 0.8479 - val_loss: 0.0350 - val_accuracy: 0.8572 Epoch 50/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0358 - accuracy: 0.8484 - val_loss: 0.0348 - val_accuracy: 0.8578 Epoch 51/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0356 - accuracy: 0.8490 - val_loss: 0.0346 - val_accuracy: 0.8583 Epoch 52/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0354 - accuracy: 0.8497 - val_loss: 0.0344 - val_accuracy: 0.8587 Epoch 53/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0352 - accuracy: 0.8504 - val_loss: 0.0342 - val_accuracy: 0.8593 Epoch 54/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0350 - accuracy: 0.8510 - val_loss: 0.0340 - val_accuracy: 0.8595 Epoch 55/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0348 - accuracy: 0.8516 - val_loss: 0.0338 - val_accuracy: 0.8604 Epoch 56/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0346 - accuracy: 0.8522 - val_loss: 0.0336 - val_accuracy: 0.8605 Epoch 57/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0344 - accuracy: 0.8526 - val_loss: 0.0334 - val_accuracy: 0.8608 Epoch 58/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0342 - accuracy: 0.8531 - val_loss: 0.0332 - val_accuracy: 0.8611 Epoch 59/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0340 - accuracy: 0.8536 - val_loss: 0.0331 - val_accuracy: 0.8617 Epoch 60/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0339 - accuracy: 0.8540 - val_loss: 0.0329 - val_accuracy: 0.8619 Epoch 61/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0337 - accuracy: 0.8547 - val_loss: 0.0327 - val_accuracy: 0.8621 Epoch 62/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0336 - accuracy: 0.8549 - val_loss: 0.0326 - val_accuracy: 0.8627 Epoch 63/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0334 - accuracy: 0.8553 - val_loss: 0.0324 - val_accuracy: 0.8635 Epoch 64/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0332 - accuracy: 0.8556 - val_loss: 0.0323 - val_accuracy: 0.8637 Epoch 65/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0331 - accuracy: 0.8562 - val_loss: 0.0321 - val_accuracy: 0.8634 Epoch 66/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0329 - accuracy: 0.8563 - val_loss: 0.0320 - val_accuracy: 0.8636 Epoch 67/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0328 - accuracy: 0.8565 - val_loss: 0.0319 - val_accuracy: 0.8642 Epoch 68/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0327 - accuracy: 0.8572 - val_loss: 0.0317 - val_accuracy: 0.8648 Epoch 69/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0325 - accuracy: 0.8575 - val_loss: 0.0316 - val_accuracy: 0.8653 Epoch 70/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0324 - accuracy: 0.8577 - val_loss: 0.0315 - val_accuracy: 0.8660 Epoch 71/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0323 - accuracy: 0.8582 - val_loss: 0.0313 - val_accuracy: 0.8662 Epoch 72/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0322 - accuracy: 0.8586 - val_loss: 0.0312 - val_accuracy: 0.8663 Epoch 73/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0320 - accuracy: 0.8590 - val_loss: 0.0311 - val_accuracy: 0.8668 Epoch 74/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0319 - accuracy: 0.8594 - val_loss: 0.0310 - val_accuracy: 0.8674 Epoch 75/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0318 - accuracy: 0.8598 - val_loss: 0.0309 - val_accuracy: 0.8676 Epoch 76/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0317 - accuracy: 0.8602 - val_loss: 0.0307 - val_accuracy: 0.8679 Epoch 77/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0316 - accuracy: 0.8605 - val_loss: 0.0306 - val_accuracy: 0.8688 Epoch 78/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0315 - accuracy: 0.8609 - val_loss: 0.0305 - val_accuracy: 0.8696 Epoch 79/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0313 - accuracy: 0.8613 - val_loss: 0.0304 - val_accuracy: 0.8702 Epoch 80/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0312 - accuracy: 0.8618 - val_loss: 0.0303 - val_accuracy: 0.8707 Epoch 81/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0311 - accuracy: 0.8622 - val_loss: 0.0302 - val_accuracy: 0.8713 Epoch 82/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0310 - accuracy: 0.8623 - val_loss: 0.0301 - val_accuracy: 0.8719 Epoch 83/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0309 - accuracy: 0.8626 - val_loss: 0.0300 - val_accuracy: 0.8723 Epoch 84/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0308 - accuracy: 0.8631 - val_loss: 0.0299 - val_accuracy: 0.8728 Epoch 85/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0307 - accuracy: 0.8635 - val_loss: 0.0298 - val_accuracy: 0.8730 Epoch 86/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0306 - accuracy: 0.8638 - val_loss: 0.0297 - val_accuracy: 0.8730 Epoch 87/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0306 - accuracy: 0.8641 - val_loss: 0.0296 - val_accuracy: 0.8733 Epoch 88/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0305 - accuracy: 0.8645 - val_loss: 0.0295 - val_accuracy: 0.8736 Epoch 89/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0304 - accuracy: 0.8646 - val_loss: 0.0294 - val_accuracy: 0.8738 Epoch 90/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0303 - accuracy: 0.8650 - val_loss: 0.0294 - val_accuracy: 0.8743 Epoch 91/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0302 - accuracy: 0.8651 - val_loss: 0.0293 - val_accuracy: 0.8745 Epoch 92/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0301 - accuracy: 0.8655 - val_loss: 0.0292 - val_accuracy: 0.8746 Epoch 93/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0300 - accuracy: 0.8657 - val_loss: 0.0291 - val_accuracy: 0.8749 Epoch 94/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0299 - accuracy: 0.8660 - val_loss: 0.0290 - val_accuracy: 0.8748 Epoch 95/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0299 - accuracy: 0.8663 - val_loss: 0.0289 - val_accuracy: 0.8752 Epoch 96/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0298 - accuracy: 0.8666 - val_loss: 0.0289 - val_accuracy: 0.8754 Epoch 97/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0297 - accuracy: 0.8668 - val_loss: 0.0288 - val_accuracy: 0.8756 Epoch 98/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0296 - accuracy: 0.8673 - val_loss: 0.0287 - val_accuracy: 0.8759 Epoch 99/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0296 - accuracy: 0.8675 - val_loss: 0.0286 - val_accuracy: 0.8762 Epoch 100/150 59/59 [==============================] - 0s 8ms/step - loss: 0.0295 - accuracy: 0.8676 - val_loss: 0.0286 - val_accuracy: 0.8766 Epoch 101/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0294 - accuracy: 0.8678 - val_loss: 0.0285 - val_accuracy: 0.8769 Epoch 102/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0293 - accuracy: 0.8681 - val_loss: 0.0284 - val_accuracy: 0.8771 Epoch 103/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0293 - accuracy: 0.8683 - val_loss: 0.0283 - val_accuracy: 0.8775 Epoch 104/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0292 - accuracy: 0.8684 - val_loss: 0.0283 - val_accuracy: 0.8776 Epoch 105/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0291 - accuracy: 0.8687 - val_loss: 0.0282 - val_accuracy: 0.8778 Epoch 106/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0291 - accuracy: 0.8689 - val_loss: 0.0281 - val_accuracy: 0.8780 Epoch 107/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0290 - accuracy: 0.8691 - val_loss: 0.0281 - val_accuracy: 0.8780 Epoch 108/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0289 - accuracy: 0.8694 - val_loss: 0.0280 - val_accuracy: 0.8786 Epoch 109/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0289 - accuracy: 0.8695 - val_loss: 0.0279 - val_accuracy: 0.8787 Epoch 110/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0288 - accuracy: 0.8697 - val_loss: 0.0279 - val_accuracy: 0.8789 Epoch 111/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0287 - accuracy: 0.8698 - val_loss: 0.0278 - val_accuracy: 0.8790 Epoch 112/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0287 - accuracy: 0.8700 - val_loss: 0.0278 - val_accuracy: 0.8795 Epoch 113/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0286 - accuracy: 0.8701 - val_loss: 0.0277 - val_accuracy: 0.8795 Epoch 114/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0285 - accuracy: 0.8703 - val_loss: 0.0276 - val_accuracy: 0.8795 Epoch 115/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0285 - accuracy: 0.8702 - val_loss: 0.0276 - val_accuracy: 0.8796 Epoch 116/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0284 - accuracy: 0.8704 - val_loss: 0.0275 - val_accuracy: 0.8798 Epoch 117/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0284 - accuracy: 0.8706 - val_loss: 0.0275 - val_accuracy: 0.8800 Epoch 118/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0283 - accuracy: 0.8708 - val_loss: 0.0274 - val_accuracy: 0.8801 Epoch 119/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0283 - accuracy: 0.8709 - val_loss: 0.0273 - val_accuracy: 0.8805 Epoch 120/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0282 - accuracy: 0.8712 - val_loss: 0.0273 - val_accuracy: 0.8806 Epoch 121/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0281 - accuracy: 0.8713 - val_loss: 0.0272 - val_accuracy: 0.8809 Epoch 122/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0281 - accuracy: 0.8716 - val_loss: 0.0272 - val_accuracy: 0.8810 Epoch 123/150 59/59 [==============================] - 0s 8ms/step - loss: 0.0280 - accuracy: 0.8719 - val_loss: 0.0271 - val_accuracy: 0.8814 Epoch 124/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0280 - accuracy: 0.8720 - val_loss: 0.0271 - val_accuracy: 0.8815 Epoch 125/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0279 - accuracy: 0.8723 - val_loss: 0.0270 - val_accuracy: 0.8818 Epoch 126/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0279 - accuracy: 0.8725 - val_loss: 0.0270 - val_accuracy: 0.8818 Epoch 127/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0278 - accuracy: 0.8727 - val_loss: 0.0269 - val_accuracy: 0.8819 Epoch 128/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0278 - accuracy: 0.8728 - val_loss: 0.0269 - val_accuracy: 0.8819 Epoch 129/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0277 - accuracy: 0.8733 - val_loss: 0.0268 - val_accuracy: 0.8820 Epoch 130/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0277 - accuracy: 0.8733 - val_loss: 0.0268 - val_accuracy: 0.8820 Epoch 131/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0276 - accuracy: 0.8735 - val_loss: 0.0267 - val_accuracy: 0.8821 Epoch 132/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0276 - accuracy: 0.8736 - val_loss: 0.0267 - val_accuracy: 0.8825 Epoch 133/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0275 - accuracy: 0.8737 - val_loss: 0.0266 - val_accuracy: 0.8826 Epoch 134/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0275 - accuracy: 0.8740 - val_loss: 0.0266 - val_accuracy: 0.8826 Epoch 135/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0274 - accuracy: 0.8741 - val_loss: 0.0265 - val_accuracy: 0.8830 Epoch 136/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0274 - accuracy: 0.8742 - val_loss: 0.0265 - val_accuracy: 0.8830 Epoch 137/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0273 - accuracy: 0.8744 - val_loss: 0.0264 - val_accuracy: 0.8834 Epoch 138/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0273 - accuracy: 0.8745 - val_loss: 0.0264 - val_accuracy: 0.8837 Epoch 139/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0273 - accuracy: 0.8747 - val_loss: 0.0263 - val_accuracy: 0.8836 Epoch 140/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0272 - accuracy: 0.8748 - val_loss: 0.0263 - val_accuracy: 0.8839 Epoch 141/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0272 - accuracy: 0.8751 - val_loss: 0.0263 - val_accuracy: 0.8842 Epoch 142/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0271 - accuracy: 0.8751 - val_loss: 0.0262 - val_accuracy: 0.8843 Epoch 143/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0271 - accuracy: 0.8752 - val_loss: 0.0262 - val_accuracy: 0.8844 Epoch 144/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0270 - accuracy: 0.8753 - val_loss: 0.0261 - val_accuracy: 0.8844 Epoch 145/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0270 - accuracy: 0.8754 - val_loss: 0.0261 - val_accuracy: 0.8843 Epoch 146/150 59/59 [==============================] - 0s 8ms/step - loss: 0.0270 - accuracy: 0.8755 - val_loss: 0.0260 - val_accuracy: 0.8843 Epoch 147/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0269 - accuracy: 0.8757 - val_loss: 0.0260 - val_accuracy: 0.8844 Epoch 148/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0269 - accuracy: 0.8758 - val_loss: 0.0260 - val_accuracy: 0.8844 Epoch 149/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0268 - accuracy: 0.8759 - val_loss: 0.0259 - val_accuracy: 0.8847 Epoch 150/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0268 - accuracy: 0.8760 - val_loss: 0.0259 - val_accuracy: 0.8849 . . Learning Curve . Para visualisar a etapa de terinamento, iremos mostrar a chamada Learning Curve, em português, curva de aprendizado. Ela consiste em mostrar o custo por época, bem como a taxa de acerto (acurácia) por época. . fig, axes = plt.subplots(1, 2, figsize=(15, 5)) axes[0].plot(100 * np.array(hist1.history[&#39;accuracy&#39;]), label=&#39;Treino&#39;) axes[0].plot(100 * np.array(hist1.history[&#39;val_accuracy&#39;]), label=&#39;Teste&#39;) axes[0].set_ylabel(&#39;Percentual de Acerto&#39;) axes[0].set_xlabel(&#39;Época&#39;) axes[0].legend() axes[1].plot(100 * np.array(hist1.history[&#39;loss&#39;]), label=&#39;Treino&#39;) axes[1].plot(100 * np.array(hist1.history[&#39;val_loss&#39;]), label=&#39;Teste&#39;) axes[1].set_ylabel(&#39;Função de Erro&#39;) axes[1].set_xlabel(&#39;Época&#39;) axes[1].legend() . &lt;matplotlib.legend.Legend at 0x7f32d2c30d30&gt; . Visualiza&#231;&#227;o do modelo . Podemos ter um olhar mais profundo sobre a rede neural ao visualisarmos seus pesos. Especialmente, nossa atenção será voltada para a matriz de pesos $W$. Note que $W$ é uma matriz $d times K$, onde $d$ é o número de características, e $K$ é o número de classes. . Podemos dizer que cada valor $W_{ij}$ dá a relevância de cada pixel $ij$ da matriz. Além disso, note que podemos dividir $W in mathbb{R}^{d times K}$, como $K$ vetores $d-$dimensionais, . $$ W = [W_{1}, cdots, W_{K}] $$Note ainda que como nós definimos $d = 28 times 28$, podemos re-transformar os pesos em uma imagem através do método $.reshape$. Isso nos permitirá uma visualização concreta das matrizes de peso. Atente que as zonas em vermelho mostram valores positivos, em azul mostram valores negativos, e próximos ao verde mostram valores próximos de zero. . W = model1.layers[1].weights[0].numpy() # b = model.layers[1].weights[1].numpy() fig, axes = plt.subplots(3, 3, figsize=(10, 10)) for i, ax in enumerate(axes.flatten()): ax.imshow(W[:, i].reshape(28, 28), cmap=&#39;jet&#39;) ax.set_xticks([]) ax.set_yticks([]) plt.savefig(&#39;single_layer_weights.pdf&#39;) . Há ainda muito ruído nos pesos da rede. Mas de maneira um pouco forçosa podemos dizer que cada peso corresponde à um &quot;protótipo&quot; de um dígito. Ou seja, cada neurônio se especializa no aprendizado de um único dígito. A próxima seção vai tratar da aprendizagem com regularização, que vai tornar esta última afirmação mais evidente. . Treinamento Perceptron Simples + Regulariza&#231;&#227;o . Regularização é uma técnica de combate ao overfitting, um fenômeno que acontece em modelos preditivos onde o modelo aprende &quot;bem demais&quot; os dados de treinamento: o modelo é tão complexo que consegue decorar os exemplos de entrada. Para novos dados, o modelo tem performance inferior. . Apesar de o exemplo anterior não demonstrar overfitting, sua matriz de pesos contém bastante ruído pois seus pesos não são limitados à um intervalo. Uma maneira de eliminar esse ruído e obter uma visualização melhor é através da regularização. Isso consiste em adiconar uma penalização à função de custo, . $$ mathcal{L}_{reg}( mathbf{W}, mathbf{b}) = mathcal{L}( mathbf{W}, mathbf{b}) + lambda Omega( mathbf{W}) $$Nessa prática iremos demonstrar o uso da penalidade $ ell^{2}$, definida através da fórmula, . $$ Omega( mathbf{W}) = dfrac{1}{2}|| mathbf{W}||^{2}_{2}, Omega( mathbf{W}) = dfrac{1}{2} sum_{i=1}^{d} sum_{j=1}^{K}W_{ij}^{2} $$Outros termos de regularização existem. Nós utilisaremos o termo $l2$ . def perceptron_mnist_l2_reg(input_shape=(784,), n_classes=10, penalty=1e-3): x = tf.keras.layers.Input(shape=input_shape) y = tf.keras.layers.Dense(units=n_classes, kernel_regularizer=tf.keras.regularizers.l2(penalty), activation=&#39;sigmoid&#39;)(x) return tf.keras.models.Model(x, y) . # Definição do modelo model2 = perceptron_mnist_l2_reg() # 1. Instanciação do custo loss_obj = tf.keras.losses.MeanSquaredError() # 2. Instanciação do otimizador optimizer_obj = tf.keras.optimizers.SGD(learning_rate=1e-1) # 3. Compilação do modelo model2.compile( loss=loss_obj, optimizer=optimizer_obj, metrics=[&#39;accuracy&#39;] ) # 4. Treinamento hist2 = model2.fit(x=Xtr, y=ytr, batch_size=1024, epochs=150, validation_data=(Xts, yts), validation_batch_size=128) . Epoch 1/150 59/59 [==============================] - 1s 9ms/step - loss: 0.1520 - accuracy: 0.1715 - val_loss: 0.1179 - val_accuracy: 0.2478 Epoch 2/150 59/59 [==============================] - 0s 7ms/step - loss: 0.1121 - accuracy: 0.3326 - val_loss: 0.1076 - val_accuracy: 0.3823 Epoch 3/150 59/59 [==============================] - 0s 7ms/step - loss: 0.1050 - accuracy: 0.4351 - val_loss: 0.1024 - val_accuracy: 0.4560 Epoch 4/150 59/59 [==============================] - 0s 7ms/step - loss: 0.1004 - accuracy: 0.4920 - val_loss: 0.0983 - val_accuracy: 0.5088 Epoch 5/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0965 - accuracy: 0.5349 - val_loss: 0.0946 - val_accuracy: 0.5496 Epoch 6/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0931 - accuracy: 0.5712 - val_loss: 0.0913 - val_accuracy: 0.5851 Epoch 7/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0900 - accuracy: 0.5995 - val_loss: 0.0883 - val_accuracy: 0.6128 Epoch 8/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0872 - accuracy: 0.6232 - val_loss: 0.0856 - val_accuracy: 0.6335 Epoch 9/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0847 - accuracy: 0.6414 - val_loss: 0.0832 - val_accuracy: 0.6510 Epoch 10/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0824 - accuracy: 0.6583 - val_loss: 0.0810 - val_accuracy: 0.6679 Epoch 11/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0804 - accuracy: 0.6723 - val_loss: 0.0791 - val_accuracy: 0.6819 Epoch 12/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0787 - accuracy: 0.6857 - val_loss: 0.0774 - val_accuracy: 0.6956 Epoch 13/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0771 - accuracy: 0.6976 - val_loss: 0.0758 - val_accuracy: 0.7049 Epoch 14/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0756 - accuracy: 0.7075 - val_loss: 0.0744 - val_accuracy: 0.7139 Epoch 15/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0743 - accuracy: 0.7161 - val_loss: 0.0731 - val_accuracy: 0.7241 Epoch 16/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0731 - accuracy: 0.7248 - val_loss: 0.0720 - val_accuracy: 0.7342 Epoch 17/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0720 - accuracy: 0.7330 - val_loss: 0.0709 - val_accuracy: 0.7429 Epoch 18/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0710 - accuracy: 0.7421 - val_loss: 0.0699 - val_accuracy: 0.7498 Epoch 19/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0700 - accuracy: 0.7509 - val_loss: 0.0689 - val_accuracy: 0.7598 Epoch 20/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0691 - accuracy: 0.7590 - val_loss: 0.0681 - val_accuracy: 0.7666 Epoch 21/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0683 - accuracy: 0.7665 - val_loss: 0.0672 - val_accuracy: 0.7731 Epoch 22/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0675 - accuracy: 0.7745 - val_loss: 0.0665 - val_accuracy: 0.7804 Epoch 23/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0668 - accuracy: 0.7810 - val_loss: 0.0657 - val_accuracy: 0.7884 Epoch 24/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0661 - accuracy: 0.7881 - val_loss: 0.0650 - val_accuracy: 0.7955 Epoch 25/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0654 - accuracy: 0.7935 - val_loss: 0.0644 - val_accuracy: 0.8016 Epoch 26/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0647 - accuracy: 0.7989 - val_loss: 0.0638 - val_accuracy: 0.8060 Epoch 27/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0641 - accuracy: 0.8041 - val_loss: 0.0632 - val_accuracy: 0.8098 Epoch 28/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0636 - accuracy: 0.8083 - val_loss: 0.0626 - val_accuracy: 0.8135 Epoch 29/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0630 - accuracy: 0.8118 - val_loss: 0.0620 - val_accuracy: 0.8177 Epoch 30/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0625 - accuracy: 0.8149 - val_loss: 0.0615 - val_accuracy: 0.8203 Epoch 31/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0620 - accuracy: 0.8179 - val_loss: 0.0610 - val_accuracy: 0.8233 Epoch 32/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0615 - accuracy: 0.8204 - val_loss: 0.0605 - val_accuracy: 0.8259 Epoch 33/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0610 - accuracy: 0.8226 - val_loss: 0.0601 - val_accuracy: 0.8288 Epoch 34/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0606 - accuracy: 0.8249 - val_loss: 0.0597 - val_accuracy: 0.8307 Epoch 35/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0602 - accuracy: 0.8264 - val_loss: 0.0592 - val_accuracy: 0.8327 Epoch 36/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0598 - accuracy: 0.8283 - val_loss: 0.0589 - val_accuracy: 0.8349 Epoch 37/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0594 - accuracy: 0.8296 - val_loss: 0.0585 - val_accuracy: 0.8365 Epoch 38/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0590 - accuracy: 0.8310 - val_loss: 0.0581 - val_accuracy: 0.8378 Epoch 39/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0587 - accuracy: 0.8322 - val_loss: 0.0578 - val_accuracy: 0.8384 Epoch 40/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0583 - accuracy: 0.8335 - val_loss: 0.0574 - val_accuracy: 0.8396 Epoch 41/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0580 - accuracy: 0.8349 - val_loss: 0.0571 - val_accuracy: 0.8414 Epoch 42/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0577 - accuracy: 0.8355 - val_loss: 0.0568 - val_accuracy: 0.8426 Epoch 43/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0574 - accuracy: 0.8365 - val_loss: 0.0565 - val_accuracy: 0.8433 Epoch 44/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0571 - accuracy: 0.8374 - val_loss: 0.0562 - val_accuracy: 0.8446 Epoch 45/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0569 - accuracy: 0.8385 - val_loss: 0.0560 - val_accuracy: 0.8458 Epoch 46/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0566 - accuracy: 0.8392 - val_loss: 0.0557 - val_accuracy: 0.8462 Epoch 47/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0563 - accuracy: 0.8404 - val_loss: 0.0554 - val_accuracy: 0.8463 Epoch 48/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0561 - accuracy: 0.8407 - val_loss: 0.0552 - val_accuracy: 0.8473 Epoch 49/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0559 - accuracy: 0.8417 - val_loss: 0.0550 - val_accuracy: 0.8480 Epoch 50/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0556 - accuracy: 0.8423 - val_loss: 0.0547 - val_accuracy: 0.8485 Epoch 51/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0554 - accuracy: 0.8431 - val_loss: 0.0545 - val_accuracy: 0.8492 Epoch 52/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0552 - accuracy: 0.8435 - val_loss: 0.0543 - val_accuracy: 0.8497 Epoch 53/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0550 - accuracy: 0.8442 - val_loss: 0.0541 - val_accuracy: 0.8501 Epoch 54/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0548 - accuracy: 0.8449 - val_loss: 0.0539 - val_accuracy: 0.8504 Epoch 55/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0546 - accuracy: 0.8456 - val_loss: 0.0537 - val_accuracy: 0.8517 Epoch 56/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0544 - accuracy: 0.8457 - val_loss: 0.0535 - val_accuracy: 0.8518 Epoch 57/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0542 - accuracy: 0.8462 - val_loss: 0.0533 - val_accuracy: 0.8520 Epoch 58/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0540 - accuracy: 0.8469 - val_loss: 0.0531 - val_accuracy: 0.8521 Epoch 59/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0539 - accuracy: 0.8469 - val_loss: 0.0530 - val_accuracy: 0.8530 Epoch 60/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0537 - accuracy: 0.8477 - val_loss: 0.0528 - val_accuracy: 0.8531 Epoch 61/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0535 - accuracy: 0.8480 - val_loss: 0.0527 - val_accuracy: 0.8539 Epoch 62/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0534 - accuracy: 0.8483 - val_loss: 0.0525 - val_accuracy: 0.8543 Epoch 63/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0532 - accuracy: 0.8487 - val_loss: 0.0523 - val_accuracy: 0.8549 Epoch 64/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0531 - accuracy: 0.8492 - val_loss: 0.0522 - val_accuracy: 0.8553 Epoch 65/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0529 - accuracy: 0.8497 - val_loss: 0.0521 - val_accuracy: 0.8555 Epoch 66/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0528 - accuracy: 0.8500 - val_loss: 0.0519 - val_accuracy: 0.8562 Epoch 67/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0527 - accuracy: 0.8503 - val_loss: 0.0518 - val_accuracy: 0.8567 Epoch 68/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0525 - accuracy: 0.8510 - val_loss: 0.0516 - val_accuracy: 0.8570 Epoch 69/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0524 - accuracy: 0.8512 - val_loss: 0.0515 - val_accuracy: 0.8576 Epoch 70/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0523 - accuracy: 0.8517 - val_loss: 0.0514 - val_accuracy: 0.8576 Epoch 71/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0522 - accuracy: 0.8519 - val_loss: 0.0513 - val_accuracy: 0.8581 Epoch 72/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0521 - accuracy: 0.8523 - val_loss: 0.0512 - val_accuracy: 0.8586 Epoch 73/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0519 - accuracy: 0.8524 - val_loss: 0.0510 - val_accuracy: 0.8591 Epoch 74/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0518 - accuracy: 0.8528 - val_loss: 0.0509 - val_accuracy: 0.8597 Epoch 75/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0517 - accuracy: 0.8533 - val_loss: 0.0508 - val_accuracy: 0.8599 Epoch 76/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0516 - accuracy: 0.8536 - val_loss: 0.0507 - val_accuracy: 0.8607 Epoch 77/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0515 - accuracy: 0.8538 - val_loss: 0.0506 - val_accuracy: 0.8609 Epoch 78/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0514 - accuracy: 0.8541 - val_loss: 0.0505 - val_accuracy: 0.8612 Epoch 79/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0513 - accuracy: 0.8544 - val_loss: 0.0504 - val_accuracy: 0.8610 Epoch 80/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0512 - accuracy: 0.8547 - val_loss: 0.0503 - val_accuracy: 0.8615 Epoch 81/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0511 - accuracy: 0.8549 - val_loss: 0.0502 - val_accuracy: 0.8624 Epoch 82/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0510 - accuracy: 0.8551 - val_loss: 0.0501 - val_accuracy: 0.8625 Epoch 83/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0509 - accuracy: 0.8554 - val_loss: 0.0500 - val_accuracy: 0.8625 Epoch 84/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0509 - accuracy: 0.8558 - val_loss: 0.0500 - val_accuracy: 0.8628 Epoch 85/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0508 - accuracy: 0.8558 - val_loss: 0.0499 - val_accuracy: 0.8632 Epoch 86/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0507 - accuracy: 0.8561 - val_loss: 0.0498 - val_accuracy: 0.8634 Epoch 87/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0506 - accuracy: 0.8563 - val_loss: 0.0497 - val_accuracy: 0.8636 Epoch 88/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0505 - accuracy: 0.8565 - val_loss: 0.0496 - val_accuracy: 0.8640 Epoch 89/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0505 - accuracy: 0.8568 - val_loss: 0.0495 - val_accuracy: 0.8645 Epoch 90/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0504 - accuracy: 0.8570 - val_loss: 0.0495 - val_accuracy: 0.8649 Epoch 91/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0503 - accuracy: 0.8575 - val_loss: 0.0494 - val_accuracy: 0.8649 Epoch 92/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0502 - accuracy: 0.8576 - val_loss: 0.0493 - val_accuracy: 0.8650 Epoch 93/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0502 - accuracy: 0.8575 - val_loss: 0.0493 - val_accuracy: 0.8653 Epoch 94/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0501 - accuracy: 0.8577 - val_loss: 0.0492 - val_accuracy: 0.8658 Epoch 95/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0500 - accuracy: 0.8580 - val_loss: 0.0491 - val_accuracy: 0.8662 Epoch 96/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0500 - accuracy: 0.8585 - val_loss: 0.0491 - val_accuracy: 0.8665 Epoch 97/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0499 - accuracy: 0.8587 - val_loss: 0.0490 - val_accuracy: 0.8661 Epoch 98/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0498 - accuracy: 0.8589 - val_loss: 0.0489 - val_accuracy: 0.8662 Epoch 99/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0498 - accuracy: 0.8587 - val_loss: 0.0489 - val_accuracy: 0.8666 Epoch 100/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0497 - accuracy: 0.8590 - val_loss: 0.0488 - val_accuracy: 0.8669 Epoch 101/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0497 - accuracy: 0.8595 - val_loss: 0.0487 - val_accuracy: 0.8669 Epoch 102/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0496 - accuracy: 0.8595 - val_loss: 0.0487 - val_accuracy: 0.8672 Epoch 103/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0495 - accuracy: 0.8597 - val_loss: 0.0486 - val_accuracy: 0.8674 Epoch 104/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0495 - accuracy: 0.8600 - val_loss: 0.0486 - val_accuracy: 0.8677 Epoch 105/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0494 - accuracy: 0.8602 - val_loss: 0.0485 - val_accuracy: 0.8679 Epoch 106/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0494 - accuracy: 0.8603 - val_loss: 0.0485 - val_accuracy: 0.8680 Epoch 107/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0493 - accuracy: 0.8605 - val_loss: 0.0484 - val_accuracy: 0.8680 Epoch 108/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0493 - accuracy: 0.8604 - val_loss: 0.0484 - val_accuracy: 0.8686 Epoch 109/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0492 - accuracy: 0.8608 - val_loss: 0.0483 - val_accuracy: 0.8685 Epoch 110/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0492 - accuracy: 0.8609 - val_loss: 0.0483 - val_accuracy: 0.8688 Epoch 111/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0491 - accuracy: 0.8609 - val_loss: 0.0482 - val_accuracy: 0.8691 Epoch 112/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0491 - accuracy: 0.8611 - val_loss: 0.0482 - val_accuracy: 0.8694 Epoch 113/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0490 - accuracy: 0.8613 - val_loss: 0.0481 - val_accuracy: 0.8694 Epoch 114/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0490 - accuracy: 0.8612 - val_loss: 0.0481 - val_accuracy: 0.8695 Epoch 115/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0489 - accuracy: 0.8615 - val_loss: 0.0480 - val_accuracy: 0.8697 Epoch 116/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0489 - accuracy: 0.8615 - val_loss: 0.0480 - val_accuracy: 0.8697 Epoch 117/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0489 - accuracy: 0.8618 - val_loss: 0.0479 - val_accuracy: 0.8697 Epoch 118/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0488 - accuracy: 0.8617 - val_loss: 0.0479 - val_accuracy: 0.8700 Epoch 119/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0488 - accuracy: 0.8619 - val_loss: 0.0479 - val_accuracy: 0.8701 Epoch 120/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0487 - accuracy: 0.8620 - val_loss: 0.0478 - val_accuracy: 0.8704 Epoch 121/150 59/59 [==============================] - 0s 8ms/step - loss: 0.0487 - accuracy: 0.8621 - val_loss: 0.0478 - val_accuracy: 0.8704 Epoch 122/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0487 - accuracy: 0.8620 - val_loss: 0.0477 - val_accuracy: 0.8709 Epoch 123/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0486 - accuracy: 0.8623 - val_loss: 0.0477 - val_accuracy: 0.8709 Epoch 124/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0486 - accuracy: 0.8624 - val_loss: 0.0477 - val_accuracy: 0.8709 Epoch 125/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0485 - accuracy: 0.8624 - val_loss: 0.0476 - val_accuracy: 0.8710 Epoch 126/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0485 - accuracy: 0.8625 - val_loss: 0.0476 - val_accuracy: 0.8712 Epoch 127/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0485 - accuracy: 0.8626 - val_loss: 0.0476 - val_accuracy: 0.8711 Epoch 128/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0484 - accuracy: 0.8629 - val_loss: 0.0475 - val_accuracy: 0.8713 Epoch 129/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0484 - accuracy: 0.8629 - val_loss: 0.0475 - val_accuracy: 0.8714 Epoch 130/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0484 - accuracy: 0.8628 - val_loss: 0.0474 - val_accuracy: 0.8714 Epoch 131/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0483 - accuracy: 0.8631 - val_loss: 0.0474 - val_accuracy: 0.8715 Epoch 132/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0483 - accuracy: 0.8632 - val_loss: 0.0474 - val_accuracy: 0.8718 Epoch 133/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0483 - accuracy: 0.8631 - val_loss: 0.0474 - val_accuracy: 0.8718 Epoch 134/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0482 - accuracy: 0.8632 - val_loss: 0.0473 - val_accuracy: 0.8720 Epoch 135/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0482 - accuracy: 0.8633 - val_loss: 0.0473 - val_accuracy: 0.8719 Epoch 136/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0482 - accuracy: 0.8634 - val_loss: 0.0473 - val_accuracy: 0.8721 Epoch 137/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0482 - accuracy: 0.8635 - val_loss: 0.0472 - val_accuracy: 0.8724 Epoch 138/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0481 - accuracy: 0.8635 - val_loss: 0.0472 - val_accuracy: 0.8725 Epoch 139/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0481 - accuracy: 0.8638 - val_loss: 0.0472 - val_accuracy: 0.8728 Epoch 140/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0481 - accuracy: 0.8638 - val_loss: 0.0471 - val_accuracy: 0.8728 Epoch 141/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0480 - accuracy: 0.8638 - val_loss: 0.0471 - val_accuracy: 0.8728 Epoch 142/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0480 - accuracy: 0.8639 - val_loss: 0.0471 - val_accuracy: 0.8730 Epoch 143/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0480 - accuracy: 0.8641 - val_loss: 0.0471 - val_accuracy: 0.8732 Epoch 144/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0480 - accuracy: 0.8641 - val_loss: 0.0470 - val_accuracy: 0.8734 Epoch 145/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0479 - accuracy: 0.8642 - val_loss: 0.0470 - val_accuracy: 0.8734 Epoch 146/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0479 - accuracy: 0.8642 - val_loss: 0.0470 - val_accuracy: 0.8734 Epoch 147/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0479 - accuracy: 0.8645 - val_loss: 0.0470 - val_accuracy: 0.8737 Epoch 148/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0479 - accuracy: 0.8645 - val_loss: 0.0469 - val_accuracy: 0.8738 Epoch 149/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0478 - accuracy: 0.8646 - val_loss: 0.0469 - val_accuracy: 0.8740 Epoch 150/150 59/59 [==============================] - 0s 7ms/step - loss: 0.0478 - accuracy: 0.8646 - val_loss: 0.0469 - val_accuracy: 0.8740 . . Aqui, fica mais claro a afirmação anterior: cada neurônio se especializa no reconhecimento de um tipo específico de dígito. . fig, axes = plt.subplots(1, 2, figsize=(15, 5)) axes[0].plot(100 * np.array(hist2.history[&#39;accuracy&#39;]), label=&#39;Treino&#39;) axes[0].plot(100 * np.array(hist2.history[&#39;val_accuracy&#39;]), label=&#39;Teste&#39;) axes[0].set_ylabel(&#39;Percentual de Acerto&#39;) axes[0].set_xlabel(&#39;Época&#39;) axes[0].legend() axes[1].plot(100 * np.array(hist2.history[&#39;loss&#39;]), label=&#39;Treino&#39;) axes[1].plot(100 * np.array(hist2.history[&#39;val_loss&#39;]), label=&#39;Teste&#39;) axes[1].set_ylabel(&#39;Função de Erro&#39;) axes[1].set_xlabel(&#39;Época&#39;) axes[1].legend() . &lt;matplotlib.legend.Legend at 0x7f32d1fc7f60&gt; . W = model2.layers[1].weights[0].numpy() # b = model.layers[1].weights[1].numpy() fig, axes = plt.subplots(3, 3, figsize=(10, 10)) for i, ax in enumerate(axes.flatten()): ax.imshow(W[:, i].reshape(28, 28), cmap=&#39;jet&#39;) ax.set_xticks([]) ax.set_yticks([]) plt.savefig(&#39;single_layer_weights_reg.pdf&#39;) . Exerc&#237;cios . Substitua a definição da função de ativação de &#39;sigmoid&#39; para &#39;softmax&#39;. Quais as implicações práticas? O que acontece com o treinamento? | Tente substituir a função de custo. Troque &#39;MeanSquaredError&#39; pela &#39;CategoricalCrossEntropy&#39; (por que não &#39;BinaryCrossEntropy&#39;?). Avalie os resultados obtidos. | Perceptron V&#225;rias Camadas . def mlp_mnist(input_shape=(784,), n_classes=10, penalty=1e-3): x = tf.keras.layers.Input(shape=input_shape) y = tf.keras.layers.Dense(units=196, activation=&#39;relu&#39;, kernel_regularizer=tf.keras.regularizers.l2(penalty))(x) y = tf.keras.layers.Dense(units=49, activation=&#39;relu&#39;, kernel_regularizer=tf.keras.regularizers.l2(penalty))(y) y = tf.keras.layers.Dense(units=n_classes, activation=&#39;softmax&#39;, kernel_regularizer=tf.keras.regularizers.l2(penalty))(y) return tf.keras.models.Model(x, y) . model3 = mlp_mnist() # Print do modelo construído model3.summary() . Model: &#34;functional_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 784)] 0 _________________________________________________________________ dense_2 (Dense) (None, 196) 153860 _________________________________________________________________ dense_3 (Dense) (None, 49) 9653 _________________________________________________________________ dense_4 (Dense) (None, 10) 500 ================================================================= Total params: 164,013 Trainable params: 164,013 Non-trainable params: 0 _________________________________________________________________ . # Passo à passo: # 1. Instancie a função de custo (num primeiro momento, use MeanSquaredError) # 2. Instancie o otimizador (SGD, ou Stochastic Gradient Descent) # 3. Compile o modelo. # 1. Instanciação do custo loss_obj = tf.keras.losses.MeanSquaredError() # 2. Instanciação do otimizador optimizer_obj = tf.keras.optimizers.SGD(learning_rate=1e-1) # 3. Compilação do modelo model3.compile( loss=loss_obj, optimizer=optimizer_obj, metrics=[&#39;accuracy&#39;] ) . hist3 = model3.fit(x=Xtr, y=ytr, batch_size=1024, epochs=150, validation_data=(Xts, yts), validation_batch_size=128) . Epoch 1/150 59/59 [==============================] - 2s 26ms/step - loss: 0.4952 - accuracy: 0.1197 - val_loss: 0.4894 - val_accuracy: 0.1482 Epoch 2/150 59/59 [==============================] - 1s 21ms/step - loss: 0.4840 - accuracy: 0.1986 - val_loss: 0.4784 - val_accuracy: 0.2632 Epoch 3/150 59/59 [==============================] - 1s 22ms/step - loss: 0.4731 - accuracy: 0.3024 - val_loss: 0.4675 - val_accuracy: 0.3404 Epoch 4/150 59/59 [==============================] - 1s 22ms/step - loss: 0.4622 - accuracy: 0.3571 - val_loss: 0.4565 - val_accuracy: 0.3814 Epoch 5/150 59/59 [==============================] - 1s 23ms/step - loss: 0.4512 - accuracy: 0.3882 - val_loss: 0.4455 - val_accuracy: 0.4091 Epoch 6/150 59/59 [==============================] - 1s 22ms/step - loss: 0.4403 - accuracy: 0.4164 - val_loss: 0.4347 - val_accuracy: 0.4351 Epoch 7/150 59/59 [==============================] - 1s 22ms/step - loss: 0.4296 - accuracy: 0.4446 - val_loss: 0.4242 - val_accuracy: 0.4590 Epoch 8/150 59/59 [==============================] - 1s 21ms/step - loss: 0.4192 - accuracy: 0.4691 - val_loss: 0.4138 - val_accuracy: 0.4792 Epoch 9/150 59/59 [==============================] - 1s 21ms/step - loss: 0.4090 - accuracy: 0.4886 - val_loss: 0.4036 - val_accuracy: 0.4965 Epoch 10/150 59/59 [==============================] - 1s 21ms/step - loss: 0.3990 - accuracy: 0.5055 - val_loss: 0.3938 - val_accuracy: 0.5135 Epoch 11/150 59/59 [==============================] - 1s 21ms/step - loss: 0.3893 - accuracy: 0.5271 - val_loss: 0.3841 - val_accuracy: 0.5397 Epoch 12/150 59/59 [==============================] - 1s 21ms/step - loss: 0.3799 - accuracy: 0.5558 - val_loss: 0.3748 - val_accuracy: 0.5697 Epoch 13/150 59/59 [==============================] - 1s 21ms/step - loss: 0.3708 - accuracy: 0.5899 - val_loss: 0.3657 - val_accuracy: 0.6048 Epoch 14/150 59/59 [==============================] - 1s 21ms/step - loss: 0.3619 - accuracy: 0.6228 - val_loss: 0.3569 - val_accuracy: 0.6394 Epoch 15/150 59/59 [==============================] - 1s 21ms/step - loss: 0.3532 - accuracy: 0.6535 - val_loss: 0.3482 - val_accuracy: 0.6675 Epoch 16/150 59/59 [==============================] - 1s 21ms/step - loss: 0.3447 - accuracy: 0.6791 - val_loss: 0.3398 - val_accuracy: 0.6926 Epoch 17/150 59/59 [==============================] - 1s 21ms/step - loss: 0.3364 - accuracy: 0.7030 - val_loss: 0.3317 - val_accuracy: 0.7151 Epoch 18/150 59/59 [==============================] - 1s 21ms/step - loss: 0.3285 - accuracy: 0.7243 - val_loss: 0.3238 - val_accuracy: 0.7354 Epoch 19/150 59/59 [==============================] - 1s 21ms/step - loss: 0.3207 - accuracy: 0.7405 - val_loss: 0.3162 - val_accuracy: 0.7479 Epoch 20/150 59/59 [==============================] - 1s 21ms/step - loss: 0.3133 - accuracy: 0.7523 - val_loss: 0.3089 - val_accuracy: 0.7607 Epoch 21/150 59/59 [==============================] - 1s 21ms/step - loss: 0.3062 - accuracy: 0.7615 - val_loss: 0.3019 - val_accuracy: 0.7716 Epoch 22/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2994 - accuracy: 0.7695 - val_loss: 0.2952 - val_accuracy: 0.7790 Epoch 23/150 59/59 [==============================] - 1s 22ms/step - loss: 0.2928 - accuracy: 0.7758 - val_loss: 0.2888 - val_accuracy: 0.7851 Epoch 24/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2864 - accuracy: 0.7815 - val_loss: 0.2825 - val_accuracy: 0.7905 Epoch 25/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2802 - accuracy: 0.7861 - val_loss: 0.2764 - val_accuracy: 0.7955 Epoch 26/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2743 - accuracy: 0.7943 - val_loss: 0.2705 - val_accuracy: 0.8047 Epoch 27/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2685 - accuracy: 0.8060 - val_loss: 0.2648 - val_accuracy: 0.8159 Epoch 28/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2629 - accuracy: 0.8163 - val_loss: 0.2593 - val_accuracy: 0.8259 Epoch 29/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2575 - accuracy: 0.8238 - val_loss: 0.2540 - val_accuracy: 0.8362 Epoch 30/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2523 - accuracy: 0.8324 - val_loss: 0.2489 - val_accuracy: 0.8416 Epoch 31/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2473 - accuracy: 0.8374 - val_loss: 0.2439 - val_accuracy: 0.8482 Epoch 32/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2424 - accuracy: 0.8426 - val_loss: 0.2391 - val_accuracy: 0.8525 Epoch 33/150 59/59 [==============================] - 1s 22ms/step - loss: 0.2377 - accuracy: 0.8469 - val_loss: 0.2345 - val_accuracy: 0.8555 Epoch 34/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2331 - accuracy: 0.8502 - val_loss: 0.2299 - val_accuracy: 0.8593 Epoch 35/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2286 - accuracy: 0.8531 - val_loss: 0.2255 - val_accuracy: 0.8625 Epoch 36/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2243 - accuracy: 0.8558 - val_loss: 0.2213 - val_accuracy: 0.8655 Epoch 37/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2201 - accuracy: 0.8581 - val_loss: 0.2171 - val_accuracy: 0.8670 Epoch 38/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2161 - accuracy: 0.8608 - val_loss: 0.2131 - val_accuracy: 0.8682 Epoch 39/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2121 - accuracy: 0.8630 - val_loss: 0.2092 - val_accuracy: 0.8700 Epoch 40/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2082 - accuracy: 0.8643 - val_loss: 0.2054 - val_accuracy: 0.8719 Epoch 41/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2045 - accuracy: 0.8660 - val_loss: 0.2017 - val_accuracy: 0.8735 Epoch 42/150 59/59 [==============================] - 1s 21ms/step - loss: 0.2008 - accuracy: 0.8674 - val_loss: 0.1981 - val_accuracy: 0.8745 Epoch 43/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1973 - accuracy: 0.8686 - val_loss: 0.1946 - val_accuracy: 0.8751 Epoch 44/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1938 - accuracy: 0.8700 - val_loss: 0.1912 - val_accuracy: 0.8766 Epoch 45/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1904 - accuracy: 0.8708 - val_loss: 0.1878 - val_accuracy: 0.8780 Epoch 46/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1872 - accuracy: 0.8723 - val_loss: 0.1846 - val_accuracy: 0.8788 Epoch 47/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1839 - accuracy: 0.8732 - val_loss: 0.1814 - val_accuracy: 0.8796 Epoch 48/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1808 - accuracy: 0.8741 - val_loss: 0.1783 - val_accuracy: 0.8802 Epoch 49/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1778 - accuracy: 0.8748 - val_loss: 0.1753 - val_accuracy: 0.8809 Epoch 50/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1748 - accuracy: 0.8755 - val_loss: 0.1724 - val_accuracy: 0.8810 Epoch 51/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1719 - accuracy: 0.8759 - val_loss: 0.1695 - val_accuracy: 0.8815 Epoch 52/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1691 - accuracy: 0.8767 - val_loss: 0.1667 - val_accuracy: 0.8820 Epoch 53/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1663 - accuracy: 0.8773 - val_loss: 0.1640 - val_accuracy: 0.8827 Epoch 54/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1637 - accuracy: 0.8780 - val_loss: 0.1614 - val_accuracy: 0.8828 Epoch 55/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1610 - accuracy: 0.8783 - val_loss: 0.1588 - val_accuracy: 0.8839 Epoch 56/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1585 - accuracy: 0.8786 - val_loss: 0.1563 - val_accuracy: 0.8846 Epoch 57/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1560 - accuracy: 0.8794 - val_loss: 0.1538 - val_accuracy: 0.8850 Epoch 58/150 59/59 [==============================] - 1s 23ms/step - loss: 0.1536 - accuracy: 0.8798 - val_loss: 0.1514 - val_accuracy: 0.8854 Epoch 59/150 59/59 [==============================] - 1s 23ms/step - loss: 0.1512 - accuracy: 0.8803 - val_loss: 0.1490 - val_accuracy: 0.8856 Epoch 60/150 59/59 [==============================] - 1s 23ms/step - loss: 0.1489 - accuracy: 0.8809 - val_loss: 0.1468 - val_accuracy: 0.8863 Epoch 61/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1466 - accuracy: 0.8811 - val_loss: 0.1445 - val_accuracy: 0.8864 Epoch 62/150 59/59 [==============================] - 1s 23ms/step - loss: 0.1444 - accuracy: 0.8814 - val_loss: 0.1423 - val_accuracy: 0.8874 Epoch 63/150 59/59 [==============================] - 1s 23ms/step - loss: 0.1422 - accuracy: 0.8820 - val_loss: 0.1402 - val_accuracy: 0.8869 Epoch 64/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1401 - accuracy: 0.8821 - val_loss: 0.1381 - val_accuracy: 0.8880 Epoch 65/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1381 - accuracy: 0.8825 - val_loss: 0.1361 - val_accuracy: 0.8878 Epoch 66/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1361 - accuracy: 0.8826 - val_loss: 0.1342 - val_accuracy: 0.8879 Epoch 67/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1341 - accuracy: 0.8829 - val_loss: 0.1322 - val_accuracy: 0.8881 Epoch 68/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1322 - accuracy: 0.8835 - val_loss: 0.1303 - val_accuracy: 0.8888 Epoch 69/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1304 - accuracy: 0.8835 - val_loss: 0.1285 - val_accuracy: 0.8889 Epoch 70/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1285 - accuracy: 0.8841 - val_loss: 0.1267 - val_accuracy: 0.8892 Epoch 71/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1268 - accuracy: 0.8841 - val_loss: 0.1249 - val_accuracy: 0.8890 Epoch 72/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1250 - accuracy: 0.8845 - val_loss: 0.1232 - val_accuracy: 0.8887 Epoch 73/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1233 - accuracy: 0.8849 - val_loss: 0.1216 - val_accuracy: 0.8893 Epoch 74/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1217 - accuracy: 0.8850 - val_loss: 0.1199 - val_accuracy: 0.8896 Epoch 75/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1201 - accuracy: 0.8853 - val_loss: 0.1183 - val_accuracy: 0.8900 Epoch 76/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1185 - accuracy: 0.8857 - val_loss: 0.1168 - val_accuracy: 0.8900 Epoch 77/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1170 - accuracy: 0.8859 - val_loss: 0.1153 - val_accuracy: 0.8899 Epoch 78/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1155 - accuracy: 0.8860 - val_loss: 0.1138 - val_accuracy: 0.8901 Epoch 79/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1140 - accuracy: 0.8860 - val_loss: 0.1123 - val_accuracy: 0.8903 Epoch 80/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1126 - accuracy: 0.8866 - val_loss: 0.1109 - val_accuracy: 0.8906 Epoch 81/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1112 - accuracy: 0.8866 - val_loss: 0.1096 - val_accuracy: 0.8904 Epoch 82/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1098 - accuracy: 0.8866 - val_loss: 0.1082 - val_accuracy: 0.8903 Epoch 83/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1085 - accuracy: 0.8868 - val_loss: 0.1069 - val_accuracy: 0.8907 Epoch 84/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1072 - accuracy: 0.8871 - val_loss: 0.1056 - val_accuracy: 0.8907 Epoch 85/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1059 - accuracy: 0.8871 - val_loss: 0.1044 - val_accuracy: 0.8913 Epoch 86/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1047 - accuracy: 0.8877 - val_loss: 0.1031 - val_accuracy: 0.8921 Epoch 87/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1035 - accuracy: 0.8877 - val_loss: 0.1019 - val_accuracy: 0.8913 Epoch 88/150 59/59 [==============================] - 1s 21ms/step - loss: 0.1023 - accuracy: 0.8877 - val_loss: 0.1008 - val_accuracy: 0.8923 Epoch 89/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1011 - accuracy: 0.8879 - val_loss: 0.0996 - val_accuracy: 0.8919 Epoch 90/150 59/59 [==============================] - 1s 22ms/step - loss: 0.1000 - accuracy: 0.8883 - val_loss: 0.0985 - val_accuracy: 0.8925 Epoch 91/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0989 - accuracy: 0.8880 - val_loss: 0.0974 - val_accuracy: 0.8927 Epoch 92/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0978 - accuracy: 0.8882 - val_loss: 0.0964 - val_accuracy: 0.8928 Epoch 93/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0968 - accuracy: 0.8884 - val_loss: 0.0953 - val_accuracy: 0.8930 Epoch 94/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0957 - accuracy: 0.8884 - val_loss: 0.0943 - val_accuracy: 0.8928 Epoch 95/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0947 - accuracy: 0.8886 - val_loss: 0.0933 - val_accuracy: 0.8929 Epoch 96/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0938 - accuracy: 0.8887 - val_loss: 0.0924 - val_accuracy: 0.8933 Epoch 97/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0928 - accuracy: 0.8888 - val_loss: 0.0914 - val_accuracy: 0.8935 Epoch 98/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0919 - accuracy: 0.8889 - val_loss: 0.0905 - val_accuracy: 0.8931 Epoch 99/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0910 - accuracy: 0.8888 - val_loss: 0.0896 - val_accuracy: 0.8935 Epoch 100/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0901 - accuracy: 0.8888 - val_loss: 0.0887 - val_accuracy: 0.8927 Epoch 101/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0892 - accuracy: 0.8888 - val_loss: 0.0879 - val_accuracy: 0.8940 Epoch 102/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0884 - accuracy: 0.8889 - val_loss: 0.0870 - val_accuracy: 0.8934 Epoch 103/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0875 - accuracy: 0.8893 - val_loss: 0.0862 - val_accuracy: 0.8938 Epoch 104/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0867 - accuracy: 0.8893 - val_loss: 0.0854 - val_accuracy: 0.8940 Epoch 105/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0859 - accuracy: 0.8894 - val_loss: 0.0847 - val_accuracy: 0.8940 Epoch 106/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0852 - accuracy: 0.8894 - val_loss: 0.0839 - val_accuracy: 0.8939 Epoch 107/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0844 - accuracy: 0.8896 - val_loss: 0.0831 - val_accuracy: 0.8936 Epoch 108/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0837 - accuracy: 0.8898 - val_loss: 0.0824 - val_accuracy: 0.8940 Epoch 109/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0830 - accuracy: 0.8896 - val_loss: 0.0817 - val_accuracy: 0.8944 Epoch 110/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0823 - accuracy: 0.8899 - val_loss: 0.0810 - val_accuracy: 0.8944 Epoch 111/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0816 - accuracy: 0.8899 - val_loss: 0.0803 - val_accuracy: 0.8940 Epoch 112/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0809 - accuracy: 0.8899 - val_loss: 0.0797 - val_accuracy: 0.8938 Epoch 113/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0802 - accuracy: 0.8898 - val_loss: 0.0790 - val_accuracy: 0.8942 Epoch 114/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0796 - accuracy: 0.8901 - val_loss: 0.0784 - val_accuracy: 0.8941 Epoch 115/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0790 - accuracy: 0.8902 - val_loss: 0.0778 - val_accuracy: 0.8942 Epoch 116/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0784 - accuracy: 0.8901 - val_loss: 0.0772 - val_accuracy: 0.8936 Epoch 117/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0778 - accuracy: 0.8902 - val_loss: 0.0766 - val_accuracy: 0.8942 Epoch 118/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0772 - accuracy: 0.8902 - val_loss: 0.0760 - val_accuracy: 0.8943 Epoch 119/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0766 - accuracy: 0.8905 - val_loss: 0.0754 - val_accuracy: 0.8941 Epoch 120/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0761 - accuracy: 0.8903 - val_loss: 0.0749 - val_accuracy: 0.8940 Epoch 121/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0755 - accuracy: 0.8906 - val_loss: 0.0744 - val_accuracy: 0.8947 Epoch 122/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0750 - accuracy: 0.8906 - val_loss: 0.0738 - val_accuracy: 0.8934 Epoch 123/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0745 - accuracy: 0.8904 - val_loss: 0.0733 - val_accuracy: 0.8944 Epoch 124/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0740 - accuracy: 0.8910 - val_loss: 0.0728 - val_accuracy: 0.8948 Epoch 125/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0735 - accuracy: 0.8908 - val_loss: 0.0723 - val_accuracy: 0.8936 Epoch 126/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0730 - accuracy: 0.8906 - val_loss: 0.0719 - val_accuracy: 0.8944 Epoch 127/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0725 - accuracy: 0.8909 - val_loss: 0.0714 - val_accuracy: 0.8938 Epoch 128/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0721 - accuracy: 0.8911 - val_loss: 0.0709 - val_accuracy: 0.8943 Epoch 129/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0716 - accuracy: 0.8910 - val_loss: 0.0705 - val_accuracy: 0.8941 Epoch 130/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0712 - accuracy: 0.8909 - val_loss: 0.0701 - val_accuracy: 0.8945 Epoch 131/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0707 - accuracy: 0.8912 - val_loss: 0.0696 - val_accuracy: 0.8936 Epoch 132/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0703 - accuracy: 0.8913 - val_loss: 0.0692 - val_accuracy: 0.8938 Epoch 133/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0699 - accuracy: 0.8914 - val_loss: 0.0688 - val_accuracy: 0.8938 Epoch 134/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0695 - accuracy: 0.8910 - val_loss: 0.0684 - val_accuracy: 0.8944 Epoch 135/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0691 - accuracy: 0.8914 - val_loss: 0.0680 - val_accuracy: 0.8945 Epoch 136/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0687 - accuracy: 0.8912 - val_loss: 0.0677 - val_accuracy: 0.8947 Epoch 137/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0684 - accuracy: 0.8914 - val_loss: 0.0673 - val_accuracy: 0.8946 Epoch 138/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0680 - accuracy: 0.8911 - val_loss: 0.0669 - val_accuracy: 0.8945 Epoch 139/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0676 - accuracy: 0.8914 - val_loss: 0.0666 - val_accuracy: 0.8939 Epoch 140/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0673 - accuracy: 0.8913 - val_loss: 0.0662 - val_accuracy: 0.8943 Epoch 141/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0669 - accuracy: 0.8916 - val_loss: 0.0659 - val_accuracy: 0.8942 Epoch 142/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0666 - accuracy: 0.8910 - val_loss: 0.0656 - val_accuracy: 0.8947 Epoch 143/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0663 - accuracy: 0.8912 - val_loss: 0.0652 - val_accuracy: 0.8944 Epoch 144/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0660 - accuracy: 0.8911 - val_loss: 0.0649 - val_accuracy: 0.8944 Epoch 145/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0657 - accuracy: 0.8913 - val_loss: 0.0646 - val_accuracy: 0.8950 Epoch 146/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0654 - accuracy: 0.8914 - val_loss: 0.0643 - val_accuracy: 0.8945 Epoch 147/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0651 - accuracy: 0.8912 - val_loss: 0.0640 - val_accuracy: 0.8946 Epoch 148/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0648 - accuracy: 0.8914 - val_loss: 0.0638 - val_accuracy: 0.8949 Epoch 149/150 59/59 [==============================] - 1s 22ms/step - loss: 0.0645 - accuracy: 0.8915 - val_loss: 0.0635 - val_accuracy: 0.8950 Epoch 150/150 59/59 [==============================] - 1s 21ms/step - loss: 0.0642 - accuracy: 0.8917 - val_loss: 0.0632 - val_accuracy: 0.8948 . . fig, axes = plt.subplots(1, 2, figsize=(15, 5)) axes[0].plot(100 * np.array(hist3.history[&#39;accuracy&#39;]), label=&#39;Treino&#39;) axes[0].plot(100 * np.array(hist3.history[&#39;val_accuracy&#39;]), label=&#39;Teste&#39;) axes[0].set_ylabel(&#39;Percentual de Acerto&#39;) axes[0].set_xlabel(&#39;Época&#39;) axes[0].legend() axes[1].plot(100 * np.array(hist3.history[&#39;loss&#39;]), label=&#39;Treino&#39;) axes[1].plot(100 * np.array(hist3.history[&#39;val_loss&#39;]), label=&#39;Teste&#39;) axes[1].set_ylabel(&#39;Função de Erro&#39;) axes[1].set_xlabel(&#39;Época&#39;) axes[1].legend() . &lt;matplotlib.legend.Legend at 0x7f32d3c82198&gt; . fig, axes = plt.subplots(14, 14, figsize=(16, 16)) for k, ax in enumerate(axes.flatten()): w = model3.layers[1].weights[0].numpy() ax.imshow(w[:, k].reshape(28, 28), cmap=&#39;jet&#39;) ax.set_xticks([]) ax.set_yticks([]) plt.savefig(&#39;mlp_weights.pdf&#39;) . fig, axes = plt.subplots(7, 7, figsize=(16, 16)) for k, ax in enumerate(axes.flatten()): w = model3.layers[2].weights[0].numpy() ax.imshow(w[:, k].reshape(14, 14), cmap=&#39;jet&#39;) ax.set_xticks([]) ax.set_yticks([]) . O resultado anterior mostra que os neurônios continuam especializados na primeira camada oculta. Isso mostra uma limitação fundamental das redes neurais rasas: o seu conhecimento é concentrado. Especialmente, se a engenharia de características não é boa, os resultados adquiridos também não são satisfatórios. Essas limitações serão superadas ao utilisarmos modelos convolucionais. . Exerc&#237;cios . Teste os resultados anteriores utilisando outros números de camadas, outras funções de ativação, outros parâmetros de regularização. Qual a taxa de acerto máxima obtida para o conjunto de teste? . Comparando diferentes modelos . Para podermos comparar modelos diferentes, precisamos ser criteriosos no treinamento destes. Note que o desempenho de um modelo nas épocas iniciais é muito diferente do desempenho após convergência. Portanto, precisamos assegurar o seguinte: . Que o modelo convergiu, | Caso a convergência não seja assegurada para os diferentes modelos, fixa-se o batch_size e o número de épocas. | Note que os dois pontos são assegurados para os nossos experimentos. . Compara&#231;&#227;o durante treino . fig, axes = plt.subplots(1, 2, figsize=(15, 5)) axes[0].plot(100 * np.array(hist1.history[&#39;accuracy&#39;]), label=&#39;Uma Camada&#39;) axes[0].plot(100 * np.array(hist2.history[&#39;accuracy&#39;]), label=&#39;Uma Camada (regularizado)&#39;) axes[0].plot(100 * np.array(hist3.history[&#39;accuracy&#39;]), label=&#39;Várias Camadas&#39;) axes[0].set_ylabel(&#39;Percentual de Acerto&#39;) axes[0].set_xlabel(&#39;Época&#39;) axes[0].legend() axes[1].plot(100 * np.array(hist1.history[&#39;loss&#39;]), label=&#39;Uma Camada&#39;) axes[1].plot(100 * np.array(hist2.history[&#39;loss&#39;]), label=&#39;Uma Camada (regularizado)&#39;) axes[1].plot(100 * np.array(hist3.history[&#39;loss&#39;]), label=&#39;Várias Camadas&#39;) axes[1].set_ylabel(&#39;Função de Erro&#39;) axes[1].set_xlabel(&#39;Época&#39;) axes[1].legend() . &lt;matplotlib.legend.Legend at 0x7f32cfbc42e8&gt; . Compara&#231;&#227;o dados de teste . yp1 = model1(Xts).numpy().argmax(axis=1) yp2 = model2(Xts).numpy().argmax(axis=1) yp3 = model3(Xts).numpy().argmax(axis=1) . print(&quot;Taxa de precisão (Uma Camada): {}&quot;.format(100 * accuracy_score(y_test, yp1))) print(&quot;Taxa de precisão (Uma Camada, regularizado): {}&quot;.format(100 * accuracy_score(y_test, yp2))) print(&quot;Taxa de precisão (Várias Camadas): {}&quot;.format(100 * accuracy_score(y_test, yp3))) . Taxa de precisão (Uma Camada): 88.49000000000001 Taxa de precisão (Uma Camada, regularizado): 87.4 Taxa de precisão (Várias Camadas): 89.48 .",
            "url": "https://eddardd.github.io/my-personal-blog/deep%20learning/tensorflow/image%20classification/neural%20networks/2020/12/01/Deep-Learning-com-Tensorflow-Aula1.html",
            "relUrl": "/deep%20learning/tensorflow/image%20classification/neural%20networks/2020/12/01/Deep-Learning-com-Tensorflow-Aula1.html",
            "date": " • Dec 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "",
          "content": "Education . . Bachelor&#39;s Degree in Electronic Engineering and Industrial Informatics | September/2018 - August/2020 | . Institut National des Sciences Appliquées de Rennes, Rennes, France | GPA: 17.26/20.00 | . Thesis Title: Optimal Transport for Domain Adaptation with Applications to Bacteria Stain Classification Advisor: Fred-Maurice Ngolè Mboula . Bachelor&#39;s Degree in Computer Engineering | January/2015 - April/2021 | . Universidade Federal do Ceará, Fortaleza, Brazil | GPA: 8.95/10.00 | . Thesis Title: Cross-Domain Fault Diagnosis through Optimal Transport Advisor: Michela Mulas . Professional Experience . . Dell Lead, Fortaleza, Brazil . Scientific Researcher in Data Mining | April 2021 - Present | . Responsible for mining textual data and creating data visualizations. . Comissariat pour l’Énergie Atomique et aux Énergies Alternatives, Giff-Sur-Yvette, France . Research Intern in Artificial Intelligence | March 2020 - August 2020 | . During this internship I was in charge of implementing Optimal Transport for Domain Adaptation methods for a problem involving transfer learning on the classification of bacteria cell stain. Supervisor: Fred-Maurice Ngolè Mboula . Institut d’Electronique et des Technologies du Numérique, groupe VAADER, Rennes, France . Research Intern in Deep Learning | March 2020 - August 2020 | . During this internship I was responsible for implementing a benchmark for comparing image denoising methods. A special focus was given to deep neural network methods. The benchmark is Open Source, and its code is hosted on Github Supervisor: Florian Lemarchand . Programming Skills . . Programming Languages . Python, Matlab (+++) C++/C (++) Javascript, VHDL (+) . Toolboxes and Libraries . Python: Tensorflow (+++), Python Optimal Transport (+++), Pytorch (++) Matlab: Simulink (++), Deep Learning Toolbos (++) . Other Software . Others: Tableau, Latex, HTML . Languages . . Portuguese (Mother Tongue) English (B2/Advanced) French (B2/Advanced) . Publications . . Lemarchand, F., Montesuma, E. F., Pelcat, M., &amp; Nogues, E. (2020, May). OpenDenoising: an Extensible Benchmark for Building Comparative Studies of Image Denoisers. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 2648-2652). IEEE. [Bibtex] [Arxiv] [Code] | Montesuma, Eduardo F., Levi PSA Alencar, and Guilherme A. Barreto. Avaliação de Algoritmos de Classificação de Padrões na Detecção de Câncer do Colo do Útero. [Bibtex] [Paper] | Montesuma, E. F., &amp; Mboula, F. M. N. (2021, June). Wasserstein Barycenter Transport for Acoustic Adaptation. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 3405-3409). [IEEE Explore] [Code] | Montesuma, E., &amp; Mboula, F. (2021). Wasserstein Barycenter for Multi-Source Domain Adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 16785-16793). [The CVF Open Access] [Paper] [Supplementary] [Code] | Scholarships and Awards . . From September/2018 to Ferbuary/2020 I had the opportunity to have my double degree studies in France funded by Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES), with a BRAFITEC Scolarship .",
          "url": "https://eddardd.github.io/my-personal-blog/cv/",
          "relUrl": "/cv/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Publications",
          "content": "A summary of my recent work is available at Google Scholar. . Thesis . 2021 . Montesuma, E. F. (2021, April). Cross-Domain Fault Diagnosis through Optimal Transport [Bachelor Thesis, Universidade Federal do Ceará]. [Research Gate] [Code] [Bibtex] . Conference Papers . 2021 . Montesuma, E. F., &amp; Mboula, F. M. N. (2021, June). Wasserstein Barycenter Transport for Acoustic Adaptation. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 3405-3409). [IEEE Explore] [Code] [Bibtex] . Montesuma, E., &amp; Mboula, F. (2021). Wasserstein Barycenter for Multi-Source Domain Adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 16785-16793). [The CVF Open Access] [Paper] [Supplementary] [Code] [Bibtex] . 2020 . Lemarchand, F., Montesuma, E. F., Pelcat, M., &amp; Nogues, E. (2020, May). OpenDenoising: an Extensible Benchmark for Building Comparative Studies of Image Denoisers. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 2648-2652). IEEE. [Arxiv] [Code] [Bibtex] . 2017 . Montesuma, E., Alencar, L., &amp; Barreto, G. (2017). Avaliação de Algoritmos de Classificação de Padrões na Detecção de Câncer do Colo do Útero. In VIII Simpósio de Instrumentação e Imagens Médicas (SIIM) / VII Simpósio de Processamento de Sinais (SPS). [Paper] [Bibtex] .",
          "url": "https://eddardd.github.io/my-personal-blog/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
      ,"page8": {
          "title": "Talks and presentations",
          "content": "Lecture on Introductory Optimal Transport for Machine Learning (05/03/2020) . A link to the presentation slides may be found here. The code used for the figures and examples can be found here . Semana da Tecnologia da Informação e Comunicação 2020 (02/12/2020 and 03/12/2020) . Crash course on Deep Learning using Tensorflow (Portuguese) [Presentation] . Session 1: [Talk] [Practical Work] | Session 2: [Talk] [Practical Work] |",
          "url": "https://eddardd.github.io/my-personal-blog/talks/",
          "relUrl": "/talks/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://eddardd.github.io/my-personal-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}